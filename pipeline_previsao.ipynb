{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf5db06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const version = '3.8.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n  const BK_RE = /^https:\\/\\/cdn\\.bokeh\\.org\\/bokeh\\/(release|dev)\\/bokeh-/;\n  const PN_RE = /^https:\\/\\/cdn\\.holoviz\\.org\\/panel\\/[^/]+\\/dist\\/panel/i;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      const shouldSkip = skip.includes(escaped) || existing_scripts.includes(escaped)\n      const isBokehOrPanel = BK_RE.test(escaped) || PN_RE.test(escaped)\n      const missingOrBroken = Bokeh == null || Bokeh.Panel == null || (Bokeh.version != version && !Bokeh.versions?.has(version)) || Bokeh.versions?.get(version).Panel == null;\n      if (shouldSkip && !(isBokehOrPanel && missingOrBroken)) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.4/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.1.min.js\", \"https://cdn.holoviz.org/panel/1.8.4/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false;\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true;\n      root._bokeh_onload_callbacks = [];\n      const bokeh_loaded = Bokeh != null && ((Bokeh.version === version && Bokeh.Panel) || (Bokeh.versions?.has(version) && Bokeh.versions.get(version).Panel));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, Bokeh, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n        if (Bokeh != undefined && !reloading) {\n          const NewBokeh = root.Bokeh;\n          if (Bokeh.versions === undefined) {\n            Bokeh.versions = new Map();\n          }\n          if (NewBokeh.version !== Bokeh.version) {\n            Bokeh[NewBokeh.version] = NewBokeh;\n            Bokeh.versions.set(NewBokeh.version, NewBokeh);\n          }\n          root.Bokeh = Bokeh;\n        }\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='9cd1893e-8933-4369-bc05-998ca55d4ce7'>\n",
       "  <div id=\"ccb05ecc-f4eb-4544-aab0-dd2765da83f2\" data-root-id=\"9cd1893e-8933-4369-bc05-998ca55d4ce7\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"e03b8fe8-2d47-4b8b-b7d6-99d115e9b3ca\":{\"version\":\"3.8.1\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"0a48df7b-bd10-41d0-b47a-ddab00910385\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"78223584-1e98-4a76-9e16-8e295334f3ff\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"9cd1893e-8933-4369-bc05-998ca55d4ce7\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"0e1eab5a-9270-45dc-96ba-3fbf5a81785e\",\"attributes\":{\"plot_id\":\"9cd1893e-8933-4369-bc05-998ca55d4ce7\",\"comm_id\":\"610a3231ffae4dd1bc5fa8445f1055b0\",\"client_comm_id\":\"ccbfb9d67ddd4206ba5140e924842aca\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"holoviews.plotting.bokeh.raster.HoverModel\",\"properties\":[{\"name\":\"xy\",\"kind\":\"Any\",\"default\":null},{\"name\":\"data\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"e03b8fe8-2d47-4b8b-b7d6-99d115e9b3ca\",\"roots\":{\"9cd1893e-8933-4369-bc05-998ca55d4ce7\":\"ccb05ecc-f4eb-4544-aab0-dd2765da83f2\"},\"root_ids\":[\"9cd1893e-8933-4369-bc05-998ca55d4ce7\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(version);\n",
       "    } else if (root.Bokeh.version === version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "9cd1893e-8933-4369-bc05-998ca55d4ce7"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando e criando diretórios...\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\Series_Temporais\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\ERA5_Bruto\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\EDA\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\EDA\\visualizacoes\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\EDA\\consolidados\n",
      "OK: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\Potencia_ONS\n"
     ]
    }
   ],
   "source": [
    "#Bibliotecas e Variaveis Uteis\n",
    "#!pip install requirements.txt -r\n",
    "import geopandas as gpd\n",
    "from calendar import monthrange\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pyarrow.parquet as pa_parquet\n",
    "import cdsapi, backoff, zipfile, shutil, time\n",
    "import os, json\n",
    "import time\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import requests\n",
    "import fastparquet\n",
    "import dask.dataframe as dd\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import hvplot.pandas\n",
    "from polars import selectors as cs\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns\n",
    "\n",
    "pasta_dados = r'C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados'\n",
    "\n",
    "# 2. Lista de todas as pastas que precisam existir\n",
    "# Extraímos apenas o diretório de cada caminho de arquivo definido\n",
    "pastas_necessarias = [\n",
    "    pasta_dados,\n",
    "    os.path.join(pasta_dados, 'Series_Temporais'),\n",
    "    os.path.join(pasta_dados, 'ERA5_Bruto'),\n",
    "    os.path.join(pasta_dados, 'EDA'),\n",
    "    os.path.join(pasta_dados, 'EDA', 'visualizacoes'),\n",
    "    os.path.join(pasta_dados, 'EDA', 'consolidados'),\n",
    "    os.path.join(pasta_dados, 'Potencia_ONS'),\n",
    "]\n",
    "\n",
    "# 3. Loop para criar as pastas automaticamente\n",
    "print(\"Verificando e criando diretórios...\")\n",
    "for pasta in pastas_necessarias:\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "    print(f\"OK: {pasta}\")\n",
    "\n",
    "#Caminhos ruins\n",
    "caminho_series_temporais = os.path.join(pasta_dados, 'Series_Temporais', 'series_temporais_bruto.parquet')\n",
    "caminho_dados_estaticos = os.path.join(pasta_dados, 'Dataset_Locais.csv')\n",
    "caminho_dataset_final = os.path.join(pasta_dados, 'Dataset_Final_Para_Modelagem.parquet')\n",
    "caminho_dataset_intermediario = os.path.join(pasta_dados, 'Dataset_Intermediario.parquet')\n",
    "output_path = os.path.join(pasta_dados, 'Series_Temporais')\n",
    "\n",
    "#Limpo\n",
    "pasta_era5 = os.path.join(pasta_dados, 'ERA5_Bruto')\n",
    "serie_temporal = os.path.join(pasta_dados, 'Series_Temporais')\n",
    "tratado_era5 = os.path.join(serie_temporal,  'series_temporais_bruto.parquet')\n",
    "dataset_locais = os.path.join(pasta_dados, 'Dataset_Locais.csv')\n",
    "#dataset_locais = os.path.join(pasta_dados, 'dataset_locais_semNaN.csv')\n",
    "pot_ons = os.path.join(pasta_dados, 'Potencia_ONS')\n",
    "\n",
    "pq = pasta_dados + \"//Series_Temporais//series_temporais_*.parquet\"\n",
    "PARQUETS = glob(pq)\n",
    "\n",
    "#Limpeza e Trat dos dados\n",
    "EDA_DIR = os.path.join(pasta_dados, 'EDA') #\"/content/drive/MyDrive/PUC/IC/ColetaEra5/Dados/EDA\"\n",
    "os.makedirs(EDA_DIR, exist_ok=True)\n",
    "viz_dir = os.path.join(EDA_DIR, \"visualizacoes\")\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "percent_glob = os.path.join(EDA_DIR, \"percentis_globais.csv\")\n",
    "consolidado = os.path.join(EDA_DIR, \"consolidados\")\n",
    "global_stat = os.path.join(consolidado,\"estatisticas_globais.csv\")\n",
    "hist = os.path.join(consolidado, \"histogramas_globais.csv\")\n",
    "valid = os.path.join(consolidado, \"validacoes_detalhadas.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Variaveis Uteis\n",
    "\n",
    "# Configuração de download\n",
    "anos = range(2015, 2026); meses = [str(m).zfill(2) for m in range(1, 13)]\n",
    "#anos = [2024]; meses = ['06']\n",
    "\n",
    "subsistema_nordeste_ufs = [\n",
    "    'MA',  # Maranhão\n",
    "    'PI',  # Piauí\n",
    "    'CE',  # Ceará\n",
    "    'RN',  # Rio Grande do Norte\n",
    "    'PB',  # Paraíba\n",
    "    'PE',  # Pernambuco\n",
    "    'AL',  # Alagoas\n",
    "    'SE',  # Sergipe\n",
    "    'BA'   # Bahia\n",
    "]\n",
    "\n",
    "status_operacional = [\n",
    "    'Operação', 'Construção não iniciada', 'Construção', 'DRO'\n",
    "    ]\n",
    "#DRO -> Despacho de Requerimento de Outorga -> Entrara no longo prazo (talvez mais de 5 anos)\n",
    "#Construção não iniciada -> Entrara no médio prazo (menos de 5 anos)\n",
    "#Construção -> Entrara no curto prazo (de 1-3 anos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381da39",
   "metadata": {},
   "source": [
    "## Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb93b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2015.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2016.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2017.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2018.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2019.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2020.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2021.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2022.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2023.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2024.csv existe\n",
      "Arquivo BALANCO_ENERGIA_SUBSISTEMA_2025.csv existe\n",
      "Arquivo CARGA_ENERGIA_2015.csv existe\n",
      "Arquivo CARGA_ENERGIA_2016.csv existe\n",
      "Arquivo CARGA_ENERGIA_2017.csv existe\n",
      "Arquivo CARGA_ENERGIA_2018.csv existe\n",
      "Arquivo CARGA_ENERGIA_2019.csv existe\n",
      "Arquivo CARGA_ENERGIA_2020.csv existe\n",
      "Arquivo CARGA_ENERGIA_2021.csv existe\n",
      "Arquivo CARGA_ENERGIA_2022.csv existe\n",
      "Arquivo CARGA_ENERGIA_2023.csv existe\n",
      "Arquivo CARGA_ENERGIA_2024.csv existe\n",
      "Arquivo CARGA_ENERGIA_2025.csv existe\n",
      "Arquivo CURVA_CARGA_2015.csv existe\n",
      "Arquivo CURVA_CARGA_2016.csv existe\n",
      "Arquivo CURVA_CARGA_2017.csv existe\n",
      "Arquivo CURVA_CARGA_2018.csv existe\n",
      "Arquivo CURVA_CARGA_2019.csv existe\n",
      "Arquivo CURVA_CARGA_2020.csv existe\n",
      "Arquivo CURVA_CARGA_2021.csv existe\n",
      "Arquivo CURVA_CARGA_2022.csv existe\n",
      "Arquivo CURVA_CARGA_2023.csv existe\n",
      "Arquivo CURVA_CARGA_2024.csv existe\n",
      "Arquivo CURVA_CARGA_2025.csv existe\n",
      "\n",
      "Resumo download: {'ok': 0, 'skip': 0, 'not_found': 0, 'fail': 0}\n",
      "Pasta cache ONS: C:\\Users\\Admin\\Documents\\Puc\\IC\\Dados\\Potencia_ONS\n"
     ]
    }
   ],
   "source": [
    "##Coleta ONS\n",
    "#Fato de Capacidade fora da analise.\n",
    "\n",
    "\n",
    "BASE_S3 = \"https://ons-aws-prod-opendata.s3.amazonaws.com/dataset\"\n",
    "OUT_DIR = pot_ons\n",
    "os.makedirs(os.path.dirname(OUT_DIR), exist_ok=True)\n",
    "\n",
    "def downloadFile(anos, alvo, url_spec):\n",
    "\n",
    "    for y in anos:\n",
    "        name = f\"{alvo}_{y}.csv\"\n",
    "        url = f\"{BASE_S3}/{url_spec}/{alvo}_{y}.csv\"\n",
    "        out_path = os.path.join(OUT_DIR, name)\n",
    "        \n",
    "        \n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"Arquivo {name} existe\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        with requests.get(url, stream=True, timeout=120) as r:\n",
    "            if r.status_code == 404:\n",
    "                return \"not_found\"\n",
    "            r.raise_for_status()\n",
    "\n",
    "            tmp = out_path + \".part\"\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            os.replace(tmp, out_path)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "lst_alvos = [[\"BALANCO_ENERGIA_SUBSISTEMA\", 'balanco_energia_subsistema_ho' ],[ \"CARGA_ENERGIA\", 'carga_energia_di'], [\"CURVA_CARGA\", 'curva-carga-ho']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_workers = 6\n",
    "results = {\"ok\": 0, \"skip\": 0, \"not_found\": 0, \"fail\": 0}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = {}\n",
    "    for alvo in lst_alvos:\n",
    "        downloadFile(anos, alvo[0], alvo[1])\n",
    "\n",
    "print(\"\\nResumo download:\", results)\n",
    "print(\"Pasta cache ONS:\", os.path.abspath(OUT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e631dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "COLETA ERA5 - DADOS METEOROLOGICOS MENSAIS\n",
      "====================================================================================================\n",
      "\n",
      "1. Carregando dados dos parques...\n",
      "   Total de registros: 17758\n",
      "   Registros no NE: 17758\n",
      "   Parques únicos no NE: 2422\n",
      "\n",
      "2. Buscando capacidade instalada em CAPACIDADE_GERACAO.csv...\n",
      "   Total de registros: 5464\n",
      "   Registros eólicos no NE: 1938\n",
      "   CEGs únicos com capacidade: 946\n",
      "   Capacidade total NE: 62060.86 MW\n",
      "   Parques sem capacidade: 0\n",
      "   Parques finais: 2422\n",
      "\n",
      "3. Criando cell_keys (células do grid ERA5)...\n",
      "   Cell_keys únicos: 225\n",
      "   Células a baixar: 225\n",
      "\n",
      "   Primeiras 5 células:\n",
      "              cell_key  lat_era5  lon_era5\n",
      "0   latm9p00_lonm37p75     -9.00    -37.75\n",
      "1   latm9p25_lonm39p00     -9.25    -39.00\n",
      "2   latm9p00_lonm39p00     -9.00    -39.00\n",
      "3   latm9p00_lonm41p50     -9.00    -41.50\n",
      "4  latm12p25_lonm42p25    -12.25    -42.25\n",
      "\n",
      "   Mapeamento salvo: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\mapeamento_parques_cells.csv\n",
      "\n",
      "4. Baixando dados ERA5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Células: 100%|██████████| 225/225 [00:01<00:00, 201.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "RESUMO DO DOWNLOAD\n",
      "====================================================================================================\n",
      "\n",
      "Total esperado: 2475\n",
      "  - Downloads OK: 0\n",
      "  - Já existiam: 2475\n",
      "  - Erros: 0\n",
      "\n",
      "Arquivos salvos em: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\ERA5_Bruto/ano=YYYY/\n",
      "Próximo passo: Executar célula de Tratamento ERA5\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Coleta ERA5 - Médias Mensais por Célula do Grid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cdsapi\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import backoff\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COLETA ERA5 - DADOS METEOROLOGICOS MENSAIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ==============================================================================\n",
    "# CONSTANTES E VARIÁVEIS\n",
    "# ==============================================================================\n",
    "RES = 0.25  # Resolução do grid ERA5 (0.25°)\n",
    "\n",
    "# Variáveis a coletar (mesmas do código original)\n",
    "vars = [\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"100m_u_component_of_wind\",\n",
    "    \"100m_v_component_of_wind\",\n",
    "    \"2m_temperature\",\n",
    "    \"sea_surface_temperature\",\n",
    "    \"surface_pressure\",\n",
    "    \"total_precipitation\",\n",
    "    \"friction_velocity\"\n",
    "]\n",
    "\n",
    "# CORREÇÃO: Adicionar definição de meses que estava faltando\n",
    "meses = [str(m).zfill(2) for m in range(1, 13)]  # ['01', '02', ..., '12']\n",
    "\n",
    "lst_errors = []\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS (COPIADAS DO ORIGINAL)\n",
    "# ==============================================================================\n",
    "\n",
    "def snap_to_era5_grid(lat, lon, res=RES):\n",
    "    \"\"\"Arredonda lat/lon para o grid ERA5 mais próximo\"\"\"\n",
    "    lat_era5 = round(lat / res) * res\n",
    "    lon_era5 = round(lon / res) * res\n",
    "    return lat_era5, lon_era5\n",
    "\n",
    "def criar_cell_key(lat_era5, lon_era5):\n",
    "    \"\"\"Cria identificador único para célula do grid\"\"\"\n",
    "    lat_str = f\"latm{abs(lat_era5):.2f}\".replace(\".\", \"p\")\n",
    "    lon_str = f\"lonm{abs(lon_era5):.2f}\".replace(\".\", \"p\")\n",
    "    return f\"{lat_str}_{lon_str}\"\n",
    "\n",
    "def bbox_single_cell(lat_era5: float, lon_era5: float, res: float = RES):\n",
    "    \"\"\"Cria bounding box de uma célula do grid\"\"\"\n",
    "    half_res = res / 2.0\n",
    "    norte = lat_era5 + half_res\n",
    "    sul = lat_era5 - half_res\n",
    "    oeste = lon_era5 - half_res\n",
    "    leste = lon_era5 + half_res\n",
    "    return [norte, oeste, sul, leste]\n",
    "\n",
    "def parse_cell_key(cell_key: str):\n",
    "    \"\"\"Extrai lat/lon de um cell_key\"\"\"\n",
    "    # Ex: \"latm10p00_lonm38p75\" -> (-10.00, -38.75)\n",
    "    parts = cell_key.split(\"_\")\n",
    "    lat_str = parts[0].replace(\"latm\", \"-\").replace(\"p\", \".\")\n",
    "    lon_str = parts[1].replace(\"lonm\", \"-\").replace(\"p\", \".\")\n",
    "    return float(lat_str), float(lon_str)\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNÇÃO DE DOWNLOAD (COPIADA DO ORIGINAL)\n",
    "# ==============================================================================\n",
    "\n",
    "# Cliente CDS API\n",
    "c = cdsapi.Client()\n",
    "\n",
    "def retrieve_year_monthly_means(ano: int, bbox: list, output_file: str):\n",
    "    \"\"\"Baixa médias mensais do ERA5 para um ano e uma bbox\"\"\"\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-single-levels-monthly-means\",\n",
    "        {\n",
    "            \"product_type\": \"monthly_averaged_reanalysis\",\n",
    "            \"variable\": vars,\n",
    "            \"year\": str(ano),\n",
    "            \"month\": meses,  # ['01', '02', ..., '12']\n",
    "            \"time\": \"00:00\",\n",
    "            \"area\": bbox,\n",
    "            \"format\": \"netcdf\",\n",
    "            \"grid\": \"0.25/0.25\",\n",
    "        },\n",
    "        output_file\n",
    "    )\n",
    "\n",
    "# Função para tratar erros e fazer retry\n",
    "def sleep_por_erro(e: Exception):\n",
    "    \"\"\"Define tempo de espera baseado no erro\"\"\"\n",
    "    msg = str(e).lower()\n",
    "\n",
    "    if \"429\" in msg or \"too many requests\" in msg:\n",
    "        return 300  # 5 minutos\n",
    "    elif \"503\" in msg or \"service unavailable\" in msg:\n",
    "        return 180  # 3 minutos\n",
    "    elif \"connection\" in msg or \"timeout\" in msg:\n",
    "        return 60   # 1 minuto\n",
    "    else:\n",
    "        return 30   # 30 segundos\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=5,\n",
    "    giveup=lambda e: \"404\" in str(e).lower()\n",
    ")\n",
    "def download_com_retry(ano, bbox, output_file):\n",
    "    \"\"\"Download com retry automático\"\"\"\n",
    "    try:\n",
    "        retrieve_year_monthly_means(ano, bbox, output_file)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        sleep_time = sleep_por_erro(e)\n",
    "        print(f\"   ERRO: {e}\")\n",
    "        print(f\"   Aguardando {sleep_time}s antes de retry...\")\n",
    "        time.sleep(sleep_time)\n",
    "        raise\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAR E PREPARAR DADOS DOS PARQUES\n",
    "# ==============================================================================\n",
    "print(\"\\n1. Carregando dados dos parques...\")\n",
    "\n",
    "df_locais = pd.read_csv(dataset_locais)\n",
    "print(f\"   Total de registros: {len(df_locais)}\")\n",
    "\n",
    "# Normalizar CEG (remover pontos, último ponto vira hífen)\n",
    "def normalizar_ceg(ceg):\n",
    "    \"\"\"Normaliza CEG para buscar em CAPACIDADE_GERACAO\"\"\"\n",
    "    if pd.isna(ceg):\n",
    "        return None\n",
    "    s = str(ceg).strip().upper()\n",
    "    if s in {\"\", \"-\"}:\n",
    "        return None\n",
    "    \n",
    "    last_dot_idx = s.rfind('.')\n",
    "    if last_dot_idx != -1:\n",
    "        s = s[:last_dot_idx] + '-' + s[last_dot_idx+1:]\n",
    "    \n",
    "    s = s.replace(\".\", \"\")\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s\n",
    "\n",
    "df_locais['CEG_norm'] = df_locais['CEG'].apply(normalizar_ceg)\n",
    "\n",
    "# Filtrar apenas NE\n",
    "df_locais_ne = df_locais[df_locais['uf'].isin(subsistema_nordeste_ufs)].copy()\n",
    "print(f\"   Registros no NE: {len(df_locais_ne)}\")\n",
    "\n",
    "# Agrupar por parque (CEG único, coordenadas do parque)\n",
    "df_parques = df_locais_ne.groupby(\n",
    "    ['CEG_norm', 'nome_parque', 'longitude_parque', 'latitude_parque', 'uf'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'qtd_aerogeradores': 'first',\n",
    "    'potencia_turbina_mw': 'sum',  # Soma das turbinas do parque\n",
    "})\n",
    "\n",
    "print(f\"   Parques únicos no NE: {len(df_parques)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. BUSCAR CAPACIDADE EM CAPACIDADE_GERACAO.CSV\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Buscando capacidade instalada em CAPACIDADE_GERACAO.csv...\")\n",
    "\n",
    "arquivo_cap = os.path.join(pot_ons, \"CAPACIDADE_GERACAO.csv\")\n",
    "df_cap = pd.read_csv(arquivo_cap, sep=';')\n",
    "print(f\"   Total de registros: {len(df_cap)}\")\n",
    "\n",
    "# Filtrar apenas eólicas no NE\n",
    "df_cap_eol_ne = df_cap[\n",
    "    (df_cap['nom_tipousina'].str.upper().str.contains('EOLI', na=False)) &\n",
    "    (df_cap['nom_subsistema'].str.upper().str.contains('NORDESTE', na=False))\n",
    "].copy()\n",
    "\n",
    "print(f\"   Registros eólicos no NE: {len(df_cap_eol_ne)}\")\n",
    "\n",
    "# Normalizar CEG\n",
    "df_cap_eol_ne['ceg_norm'] = df_cap_eol_ne['ceg'].apply(normalizar_ceg)\n",
    "\n",
    "# Converter potência\n",
    "df_cap_eol_ne['val_potenciaefetiva'] = pd.to_numeric(\n",
    "    df_cap_eol_ne['val_potenciaefetiva'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Agregar capacidade por CEG\n",
    "capacidade_por_ceg = df_cap_eol_ne.groupby('ceg_norm')['val_potenciaefetiva'].sum()\n",
    "\n",
    "print(f\"   CEGs únicos com capacidade: {len(capacidade_por_ceg)}\")\n",
    "\n",
    "# Merge com df_parques\n",
    "df_parques['capacidade_mw'] = df_parques['CEG_norm'].map(capacidade_por_ceg)\n",
    "\n",
    "# CORREÇÃO: Usar fillna sem inplace para evitar FutureWarning\n",
    "df_parques['capacidade_mw'] = df_parques['capacidade_mw'].fillna(df_parques['potencia_turbina_mw'])\n",
    "\n",
    "print(f\"   Capacidade total NE: {df_parques['capacidade_mw'].sum():.2f} MW\")\n",
    "print(f\"   Parques sem capacidade: {df_parques['capacidade_mw'].isna().sum()}\")\n",
    "\n",
    "# Remover parques sem capacidade\n",
    "df_parques = df_parques[df_parques['capacidade_mw'].notna()].copy()\n",
    "\n",
    "print(f\"   Parques finais: {len(df_parques)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CRIAR CELL_KEYS (CÉLULAS DO GRID ERA5)\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Criando cell_keys (células do grid ERA5)...\")\n",
    "\n",
    "# Snap para grid ERA5\n",
    "df_parques[['lat_era5', 'lon_era5']] = df_parques.apply(\n",
    "    lambda r: pd.Series(snap_to_era5_grid(r['latitude_parque'], r['longitude_parque'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Criar cell_key\n",
    "df_parques['cell_key'] = df_parques.apply(\n",
    "    lambda r: criar_cell_key(r['lat_era5'], r['lon_era5']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"   Cell_keys únicos: {df_parques['cell_key'].nunique()}\")\n",
    "\n",
    "# DataFrame de células únicas\n",
    "df_cells = df_parques[['cell_key', 'lat_era5', 'lon_era5']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"   Células a baixar: {len(df_cells)}\")\n",
    "print(f\"\\n   Primeiras 5 células:\")\n",
    "print(df_cells.head().to_string())\n",
    "\n",
    "# Salvar mapeamento parque -> cell_key para uso posterior\n",
    "mapeamento_file = os.path.join(pasta_dados, 'mapeamento_parques_cells.csv')\n",
    "df_parques.to_csv(mapeamento_file, index=False)\n",
    "print(f\"\\n   Mapeamento salvo: {mapeamento_file}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. DOWNLOAD DOS DADOS ERA5\n",
    "# ==============================================================================\n",
    "print(\"\\n4. Baixando dados ERA5...\")\n",
    "\n",
    "# Para cada célula, baixar dados de todos os anos\n",
    "total_downloads = len(df_cells) * len(list(anos))\n",
    "downloads_ok = 0\n",
    "downloads_skip = 0\n",
    "downloads_erro = 0\n",
    "\n",
    "for _, cell_row in tqdm(df_cells.iterrows(), total=len(df_cells), desc=\"Células\"):\n",
    "    cell_key = cell_row['cell_key']\n",
    "    lat_era5 = cell_row['lat_era5']\n",
    "    lon_era5 = cell_row['lon_era5']\n",
    "\n",
    "    bbox = bbox_single_cell(lat_era5, lon_era5)\n",
    "\n",
    "    for ano in anos:\n",
    "        # Criar diretório para o ano\n",
    "        ano_dir = os.path.join(pasta_era5, f\"ano={ano}\")\n",
    "        os.makedirs(ano_dir, exist_ok=True)\n",
    "\n",
    "        # Nome do arquivo\n",
    "        output_file = os.path.join(ano_dir, f\"Era5M_NE-{cell_key}_{ano}.nc\")\n",
    "\n",
    "        # Verificar se já existe\n",
    "        if os.path.exists(output_file):\n",
    "            downloads_skip += 1\n",
    "            continue\n",
    "\n",
    "        # Download\n",
    "        try:\n",
    "            download_com_retry(ano, bbox, output_file)\n",
    "            downloads_ok += 1\n",
    "            print(f\"   OK: {cell_key} - {ano}\")\n",
    "            time.sleep(1)  # Pausa para não sobrecarregar API\n",
    "\n",
    "        except Exception as e:\n",
    "            downloads_erro += 1\n",
    "            lst_errors.append({\n",
    "                'cell_key': cell_key,\n",
    "                'ano': ano,\n",
    "                'erro': str(e)\n",
    "            })\n",
    "            print(f\"   ERRO: {cell_key} - {ano}: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. RESUMO\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RESUMO DO DOWNLOAD\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTotal esperado: {total_downloads}\")\n",
    "print(f\"  - Downloads OK: {downloads_ok}\")\n",
    "print(f\"  - Já existiam: {downloads_skip}\")\n",
    "print(f\"  - Erros: {downloads_erro}\")\n",
    "\n",
    "if lst_errors:\n",
    "    print(f\"\\n⚠️  {len(lst_errors)} downloads falharam:\")\n",
    "    df_errors = pd.DataFrame(lst_errors)\n",
    "    print(df_errors.to_string())\n",
    "\n",
    "    # Salvar erros\n",
    "    error_file = os.path.join(pasta_dados, 'erros_download_era5.csv')\n",
    "    df_errors.to_csv(error_file, index=False)\n",
    "    print(f\"\\n   Erros salvos em: {error_file}\")\n",
    "\n",
    "print(f\"\\nArquivos salvos em: {pasta_era5}/ano=YYYY/\")\n",
    "print(f\"Próximo passo: Executar célula de Tratamento ERA5\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d5a57",
   "metadata": {},
   "source": [
    "## Tratamento de Dados e Geração de Estatisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "429f69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "UNIFICACAO DOS DADOS ONS - DADOS MENSAIS AGREGADOS PARA O NE\n",
      "====================================================================================================\n",
      "\n",
      "1. Processando BALANCO_ENERGIA_SUBSISTEMA...\n",
      "   Lendo: 2015...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2016...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2017...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2018...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2019...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2020...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2021...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2022...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2023...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2024...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2025...\n",
      "      -> 12 meses processados\n",
      "\n",
      "   Total: 132 registros mensais (geracao eolica + carga NE)\n",
      "\n",
      "2. Processando CAPACIDADE_GERACAO (considerando entrada E desativação)...\n",
      "   Total de registros: 5464\n",
      "   Registros eolicos no NE: 1938\n",
      "   Registros com data de entrada: 1938\n",
      "   Registros com data de desativação: 0\n",
      "   Capacidade calculada para 132 meses\n",
      "   Capacidade em 2015-01: 3397.43 MW\n",
      "   Capacidade em 2025-12: 30238.60 MW\n",
      "\n",
      "3. Processando CARGA_ENERGIA (SIN completo)...\n",
      "   Lendo: 2015...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2016...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2017...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2018...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2019...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2020...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2021...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2022...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2023...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2024...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2025...\n",
      "      -> 12 meses processados\n",
      "\n",
      "   Total: 132 registros mensais (demanda SIN)\n",
      "\n",
      "4. Processando CURVA_CARGA...\n",
      "   Lendo: 2015...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2016...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2017...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2018...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2019...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2020...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2021...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2022...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2023...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2024...\n",
      "      -> 12 meses processados\n",
      "   Lendo: 2025...\n",
      "      -> 12 meses processados\n",
      "\n",
      "   Total: 132 registros mensais (demanda maxima SIN)\n",
      "\n",
      "5. Unificando dados...\n",
      "\n",
      "6. Calculando features derivadas...\n",
      "\n",
      "Dataset final: (132, 14)\n",
      "\n",
      "Primeiras 10 linhas:\n",
      "    ano  mes subsistema  geracao_eolica_ne_mwmed  capacidade_eolica_ne_mw  carga_ne_mwmed  demanda_sin_mwmed  demanda_max_sin_mw  corte_eolica_ne_mwmed  capacity_factor_ne  penetracao_eolica_ne  penetracao_eolica_sin  crescimento_capacidade_mw  trimestre\n",
      "0  2015    1         NE              1610.625311                3397.4252    10042.264086       67110.382256        84524.630907               0.000000           47.407234             16.038468               2.399964                       0.00          1\n",
      "1  2015    2         NE              1349.391557                3397.4252    10003.646017       65846.806979        83225.478698             179.449783           39.718065             13.488997               2.049289                       0.00          1\n",
      "2  2015    3         NE              1059.652541                3571.3252    10058.703664       64983.448215        80202.901984             547.443799           29.671130             10.534683               1.630650                     173.90          1\n",
      "3  2015    4         NE               916.722237                3748.9252    10055.286826       59368.700762        76380.998411             770.294103           24.452935              9.116818               1.544117                     177.60          2\n",
      "4  2015    5         NE              1568.733989                3807.7852     9804.350262       57981.014222        71152.415515             144.769351           41.198069             16.000387               2.705599                      58.86          2\n",
      "5  2015    6         NE              1680.084851                3969.7852     9447.666171       56778.919713        70776.984125             106.318489           42.321808             17.783067               2.958994                     162.00          2\n",
      "6  2015    7         NE              1983.367506                4011.4852     9109.736863       56950.599557        69130.691193               0.000000           49.442224             21.771952               3.482610                      41.70          3\n",
      "7  2015    8         NE              2574.252683                4186.9852     9208.784721       58282.679086        71074.159133               0.000000           61.482249             27.954315               4.416840                     175.50          3\n",
      "8  2015    9         NE              2325.043701                4270.5852     9579.984389       60537.356153        78755.460613               0.000000           54.443211             24.269807               3.840676                      83.60          3\n",
      "9  2015   10         NE              2443.732999                4362.7852     9896.937156       62217.854098        78652.598790               0.000000           56.013140             24.691811               3.927704                      92.20          4\n",
      "\n",
      "Ultimas 10 linhas:\n",
      "      ano  mes subsistema  geracao_eolica_ne_mwmed  capacidade_eolica_ne_mw  carga_ne_mwmed  demanda_sin_mwmed  demanda_max_sin_mw  corte_eolica_ne_mwmed  capacity_factor_ne  penetracao_eolica_ne  penetracao_eolica_sin  crescimento_capacidade_mw  trimestre\n",
      "122  2025    3         NE             10701.555228               29455.5952    13792.189364       85972.599249          101816.875            2553.462612           36.331146             77.591417              12.447635                       54.0          1\n",
      "123  2025    4         NE              9550.020456               29608.5952    13829.252121       79391.651619           97250.782            3773.847384           32.254217             69.056666              12.028998                      153.0          2\n",
      "124  2025    5         NE             13435.560957               29635.5952    13322.290313       76241.228894           93551.502               0.000000           45.335890            100.850234              17.622435                       27.0          2\n",
      "125  2025    6         NE             13893.463218               29671.5952    12762.132685       74620.188646           92225.860               0.000000           46.824120            108.864745              18.618907                       36.0          2\n",
      "126  2025    7         NE             14195.322844               29779.5952    12209.762642       74135.347655           92844.408               0.000000           47.667951            116.262070              19.147847                      108.0          3\n",
      "127  2025    8         NE             14398.224632               29820.0952    12449.615200       74742.558281           93038.938               0.000000           48.283631            115.651965              19.263757                       40.5          3\n",
      "128  2025    9         NE             15880.440722               29820.0952    13036.568203       77418.425695           93225.076               0.000000           53.254158            121.814579              20.512482                        0.0          3\n",
      "129  2025   10         NE             14500.941262               30220.5952    13318.657934       78692.995848           94863.335               0.000000           47.983639            108.876895              18.427232                      400.5          4\n",
      "130  2025   11         NE             11375.155126               30238.5952    13814.669012       80176.858210           96072.991            2232.212714           37.618001             82.341134              14.187579                       18.0          4\n",
      "131  2025   12         NE             10764.455866               30238.5952    13571.434660       82315.920964           97648.660            2842.911974           35.598399             79.317008              13.077004                        0.0          4\n",
      "\n",
      "Estatisticas:\n",
      "               ano         mes  geracao_eolica_ne_mwmed  \\\n",
      "count   132.000000  132.000000               132.000000   \n",
      "mean   2020.000000    6.500000              6608.205194   \n",
      "std       3.174324    3.465203              3817.207393   \n",
      "min    2015.000000    1.000000               916.722237   \n",
      "25%    2017.000000    3.750000              3298.219409   \n",
      "50%    2020.000000    6.500000              6012.275395   \n",
      "75%    2023.000000    9.250000              9103.673211   \n",
      "max    2025.000000   12.000000             15880.440722   \n",
      "\n",
      "       capacidade_eolica_ne_mw  carga_ne_mwmed  demanda_sin_mwmed  \\\n",
      "count               132.000000      132.000000         132.000000   \n",
      "mean              15065.588458    11112.386456       67863.034463   \n",
      "std                8181.657330     1278.173432        7336.615434   \n",
      "min                3397.425200     9109.736863       55955.377203   \n",
      "25%                8401.725200    10080.267773       62189.643594   \n",
      "50%               12453.305200    10837.003621       66475.744294   \n",
      "75%               21560.595200    11840.926359       72292.463535   \n",
      "max               30238.595200    13833.600750       89206.778943   \n",
      "\n",
      "       demanda_max_sin_mw  corte_eolica_ne_mwmed  capacity_factor_ne  \\\n",
      "count          132.000000             132.000000          132.000000   \n",
      "mean         83660.797365             765.777029           44.333817   \n",
      "std           8626.832788            1208.330727           11.454325   \n",
      "min          69130.691193               0.000000           18.917702   \n",
      "25%          77950.277166               0.000000           36.232859   \n",
      "50%          82806.293500              80.561537           44.395544   \n",
      "75%          88588.181000            1195.146487           53.267367   \n",
      "max         106148.662000            6047.267442           68.865279   \n",
      "\n",
      "       penetracao_eolica_ne  penetracao_eolica_sin  crescimento_capacidade_mw  \\\n",
      "count            132.000000             132.000000                 132.000000   \n",
      "mean              57.406613               9.460763                 203.342197   \n",
      "std               28.873117               4.868242                 180.239499   \n",
      "min                9.116818               1.544117                   0.000000   \n",
      "25%               33.549689               5.357924                  61.460000   \n",
      "50%               56.058407               9.061434                 167.450000   \n",
      "75%               76.771522              12.444739                 278.011250   \n",
      "max              121.814579              20.512482                 947.400000   \n",
      "\n",
      "        trimestre  \n",
      "count  132.000000  \n",
      "mean     2.500000  \n",
      "std      1.122293  \n",
      "min      1.000000  \n",
      "25%      1.750000  \n",
      "50%      2.500000  \n",
      "75%      3.250000  \n",
      "max      4.000000  \n",
      "\n",
      "Valores nulos por coluna:\n",
      "ano                          0\n",
      "mes                          0\n",
      "subsistema                   0\n",
      "geracao_eolica_ne_mwmed      0\n",
      "capacidade_eolica_ne_mw      0\n",
      "carga_ne_mwmed               0\n",
      "demanda_sin_mwmed            0\n",
      "demanda_max_sin_mw           0\n",
      "corte_eolica_ne_mwmed        0\n",
      "capacity_factor_ne           0\n",
      "penetracao_eolica_ne         0\n",
      "penetracao_eolica_sin        0\n",
      "crescimento_capacidade_mw    0\n",
      "trimestre                    0\n",
      "dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ARQUIVO SALVO: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\dados_operacao_ONS.csv\n",
      "Total de registros: 132\n",
      "Periodo: 2015-01 ate 2025-12\n",
      "\n",
      "✅ CORREÇÃO APLICADA:\n",
      "  - Capacidade agora considera entrada E desativação de usinas\n",
      "  - Apenas usinas ATIVAS em cada mês são contabilizadas\n",
      "\n",
      "Colunas criadas:\n",
      "  - ano\n",
      "  - mes\n",
      "  - subsistema\n",
      "  - geracao_eolica_ne_mwmed\n",
      "  - capacidade_eolica_ne_mw\n",
      "  - carga_ne_mwmed\n",
      "  - demanda_sin_mwmed\n",
      "  - demanda_max_sin_mw\n",
      "  - corte_eolica_ne_mwmed\n",
      "  - capacity_factor_ne\n",
      "  - penetracao_eolica_ne\n",
      "  - penetracao_eolica_sin\n",
      "  - crescimento_capacidade_mw\n",
      "  - trimestre\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Processamento ONS\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"UNIFICACAO DOS DADOS ONS - DADOS MENSAIS AGREGADOS PARA O NE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Diretorio base\n",
    "data_dir = pot_ons\n",
    "\n",
    "# Anos disponiveis\n",
    "anos_range = list(range(2015, 2026))  # 2015 a 2025\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PROCESSAR BALANCO_ENERGIA_SUBSISTEMA - Geracao eolica mensal NE\n",
    "# ==============================================================================\n",
    "print(\"\\n1. Processando BALANCO_ENERGIA_SUBSISTEMA...\")\n",
    "balanco_frames = []\n",
    "\n",
    "for ano in anos_range:\n",
    "    arquivo = os.path.join(data_dir, f\"BALANCO_ENERGIA_SUBSISTEMA_{ano}.csv\")\n",
    "    if not os.path.exists(arquivo):\n",
    "        print(f\"   [AVISO] Arquivo nao encontrado: {arquivo}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"   Lendo: {ano}...\")\n",
    "    df = pd.read_csv(arquivo, sep=';')\n",
    "\n",
    "    # Filtrar apenas NORDESTE\n",
    "    df_ne = df[df['nom_subsistema'].str.upper().str.contains('NORDESTE', na=False)].copy()\n",
    "\n",
    "    # Converter data\n",
    "    df_ne['din_instante'] = pd.to_datetime(df_ne['din_instante'], errors='coerce')\n",
    "    df_ne['ano'] = df_ne['din_instante'].dt.year\n",
    "    df_ne['mes'] = df_ne['din_instante'].dt.month\n",
    "\n",
    "    # Converter valores\n",
    "    df_ne['val_gereolica'] = pd.to_numeric(df_ne['val_gereolica'], errors='coerce')\n",
    "    df_ne['val_carga'] = pd.to_numeric(df_ne['val_carga'], errors='coerce')\n",
    "\n",
    "    # Agregar por mes\n",
    "    df_mensal = df_ne.groupby(['ano', 'mes'], as_index=False).agg({\n",
    "        'val_gereolica': 'mean',  # Media mensal em MWmed\n",
    "        'val_carga': 'mean'       # Carga media mensal do NE em MWmed\n",
    "    })\n",
    "\n",
    "    df_mensal.rename(columns={\n",
    "        'val_gereolica': 'geracao_eolica_ne_mwmed',\n",
    "        'val_carga': 'carga_ne_mwmed'\n",
    "    }, inplace=True)\n",
    "\n",
    "    balanco_frames.append(df_mensal)\n",
    "    print(f\"      -> {len(df_mensal)} meses processados\")\n",
    "\n",
    "df_balanco = pd.concat(balanco_frames, ignore_index=True)\n",
    "print(f\"\\n   Total: {len(df_balanco)} registros mensais (geracao eolica + carga NE)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PROCESSAR CAPACIDADE_GERACAO - Capacidade instalada eolica NE (COM DESATIVAÇÃO)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Processando CAPACIDADE_GERACAO (considerando entrada E desativação)...\")\n",
    "arquivo_cap = os.path.join(data_dir, \"CAPACIDADE_GERACAO.csv\")\n",
    "\n",
    "df_cap = pd.read_csv(arquivo_cap, sep=';')\n",
    "print(f\"   Total de registros: {len(df_cap)}\")\n",
    "\n",
    "# Filtrar apenas EOLICA no NORDESTE\n",
    "df_cap_eol_ne = df_cap[\n",
    "    (df_cap['nom_tipousina'].str.upper().str.contains('EOLI', na=False)) &\n",
    "    (df_cap['nom_subsistema'].str.upper().str.contains('NORDESTE', na=False))\n",
    "].copy()\n",
    "\n",
    "print(f\"   Registros eolicos no NE: {len(df_cap_eol_ne)}\")\n",
    "\n",
    "# Converter datas de ENTRADA e DESATIVAÇÃO\n",
    "df_cap_eol_ne['dat_entradaoperacao'] = pd.to_datetime(df_cap_eol_ne['dat_entradaoperacao'], errors='coerce')\n",
    "df_cap_eol_ne['dat_desativacao'] = pd.to_datetime(df_cap_eol_ne['dat_desativacao'], errors='coerce')\n",
    "\n",
    "# Converter potencia\n",
    "df_cap_eol_ne['val_potenciaefetiva'] = pd.to_numeric(df_cap_eol_ne['val_potenciaefetiva'], errors='coerce')\n",
    "\n",
    "# Remover registros sem data de entrada ou potencia\n",
    "df_cap_eol_ne = df_cap_eol_ne.dropna(subset=['dat_entradaoperacao', 'val_potenciaefetiva'])\n",
    "\n",
    "print(f\"   Registros com data de entrada: {len(df_cap_eol_ne)}\")\n",
    "print(f\"   Registros com data de desativação: {df_cap_eol_ne['dat_desativacao'].notna().sum()}\")\n",
    "\n",
    "# Criar um DataFrame com todos os meses de 2015 a 2025\n",
    "todos_meses = []\n",
    "for ano in range(2015, 2026):\n",
    "    for mes in range(1, 13):\n",
    "        todos_meses.append({'ano': ano, 'mes': mes})\n",
    "df_meses = pd.DataFrame(todos_meses)\n",
    "\n",
    "# Para cada mes, calcular capacidade de usinas ATIVAS\n",
    "capacidades = []\n",
    "for _, row in df_meses.iterrows():\n",
    "    ano_ref = row['ano']\n",
    "    mes_ref = row['mes']\n",
    "    \n",
    "    # Data de referência do mês\n",
    "    data_ref = pd.Timestamp(year=ano_ref, month=mes_ref, day=1)\n",
    "    \n",
    "    # Filtro: usina ATIVA neste mês\n",
    "    # Ativa = data_ref >= dat_entradaoperacao AND (dat_desativacao is NULL OR data_ref < dat_desativacao)\n",
    "    mask = (\n",
    "        (df_cap_eol_ne['dat_entradaoperacao'] <= data_ref) &\n",
    "        (\n",
    "            df_cap_eol_ne['dat_desativacao'].isna() |\n",
    "            (df_cap_eol_ne['dat_desativacao'] > data_ref)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    cap_ativa = df_cap_eol_ne[mask]['val_potenciaefetiva'].sum()\n",
    "    capacidades.append(cap_ativa)\n",
    "\n",
    "df_meses['capacidade_eolica_ne_mw'] = capacidades\n",
    "\n",
    "print(f\"   Capacidade calculada para {len(df_meses)} meses\")\n",
    "print(f\"   Capacidade em 2015-01: {df_meses[(df_meses['ano']==2015) & (df_meses['mes']==1)]['capacidade_eolica_ne_mw'].values[0]:.2f} MW\")\n",
    "print(f\"   Capacidade em 2025-12: {df_meses[(df_meses['ano']==2025) & (df_meses['mes']==12)]['capacidade_eolica_ne_mw'].values[0]:.2f} MW\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESSAR CARGA_ENERGIA - Demanda mensal SIN completo\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Processando CARGA_ENERGIA (SIN completo)...\")\n",
    "carga_frames = []\n",
    "\n",
    "for ano in anos_range:\n",
    "    arquivo = os.path.join(data_dir, f\"CARGA_ENERGIA_{ano}.csv\")\n",
    "    if not os.path.exists(arquivo):\n",
    "        print(f\"   [AVISO] Arquivo nao encontrado: {arquivo}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"   Lendo: {ano}...\")\n",
    "    df = pd.read_csv(arquivo, sep=';')\n",
    "\n",
    "    # Converter data\n",
    "    df['din_instante'] = pd.to_datetime(df['din_instante'], errors='coerce')\n",
    "    df['ano'] = df['din_instante'].dt.year\n",
    "    df['mes'] = df['din_instante'].dt.month\n",
    "\n",
    "    # Converter valor\n",
    "    df['val_cargaenergiamwmed'] = pd.to_numeric(df['val_cargaenergiamwmed'], errors='coerce')\n",
    "\n",
    "    # Agregar por dia (soma de todos subsistemas)\n",
    "    df_dia = df.groupby(['ano', 'mes', 'din_instante'], as_index=False).agg({\n",
    "        'val_cargaenergiamwmed': 'sum'\n",
    "    })\n",
    "\n",
    "    # Agregar por mes (media mensal)\n",
    "    df_mensal = df_dia.groupby(['ano', 'mes'], as_index=False).agg({\n",
    "        'val_cargaenergiamwmed': 'mean'\n",
    "    })\n",
    "\n",
    "    df_mensal.rename(columns={'val_cargaenergiamwmed': 'demanda_sin_mwmed'}, inplace=True)\n",
    "    carga_frames.append(df_mensal)\n",
    "    print(f\"      -> {len(df_mensal)} meses processados\")\n",
    "\n",
    "df_carga = pd.concat(carga_frames, ignore_index=True)\n",
    "print(f\"\\n   Total: {len(df_carga)} registros mensais (demanda SIN)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PROCESSAR CURVA_CARGA - Demanda maxima mensal SIN\n",
    "# ==============================================================================\n",
    "print(\"\\n4. Processando CURVA_CARGA...\")\n",
    "curva_frames = []\n",
    "\n",
    "for ano in anos_range:\n",
    "    arquivo = os.path.join(data_dir, f\"CURVA_CARGA_{ano}.csv\")\n",
    "    if not os.path.exists(arquivo):\n",
    "        print(f\"   [AVISO] Arquivo nao encontrado: {arquivo}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"   Lendo: {ano}...\")\n",
    "    df = pd.read_csv(arquivo, sep=';')\n",
    "\n",
    "    # Converter data\n",
    "    df['din_instante'] = pd.to_datetime(df['din_instante'], errors='coerce')\n",
    "    df['ano'] = df['din_instante'].dt.year\n",
    "    df['mes'] = df['din_instante'].dt.month\n",
    "\n",
    "    # Converter valor\n",
    "    df['val_cargaenergiahomwmed'] = pd.to_numeric(df['val_cargaenergiahomwmed'], errors='coerce')\n",
    "\n",
    "    # Para cada hora, somar todos os subsistemas (SIN total)\n",
    "    df_hora = df.groupby(['ano', 'mes', 'din_instante'], as_index=False).agg({\n",
    "        'val_cargaenergiahomwmed': 'sum'\n",
    "    })\n",
    "\n",
    "    # Para cada mes, pegar o maximo (pico de demanda)\n",
    "    df_mensal = df_hora.groupby(['ano', 'mes'], as_index=False).agg({\n",
    "        'val_cargaenergiahomwmed': 'max'\n",
    "    })\n",
    "\n",
    "    df_mensal.rename(columns={'val_cargaenergiahomwmed': 'demanda_max_sin_mw'}, inplace=True)\n",
    "    curva_frames.append(df_mensal)\n",
    "    print(f\"      -> {len(df_mensal)} meses processados\")\n",
    "\n",
    "df_curva = pd.concat(curva_frames, ignore_index=True)\n",
    "print(f\"\\n   Total: {len(df_curva)} registros mensais (demanda maxima SIN)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. UNIFICAR TUDO\n",
    "# ==============================================================================\n",
    "print(\"\\n5. Unificando dados...\")\n",
    "\n",
    "# Comecar com balanco (geracao eolica + carga NE)\n",
    "df_final = df_balanco.copy()\n",
    "\n",
    "# Merge com capacidade\n",
    "df_final = df_final.merge(df_meses[['ano', 'mes', 'capacidade_eolica_ne_mw']],\n",
    "                           on=['ano', 'mes'], how='left')\n",
    "\n",
    "# Merge com carga SIN\n",
    "df_final = df_final.merge(df_carga, on=['ano', 'mes'], how='left')\n",
    "\n",
    "# Merge com curva (demanda maxima)\n",
    "df_final = df_final.merge(df_curva, on=['ano', 'mes'], how='left')\n",
    "\n",
    "# Adicionar coluna de subsistema\n",
    "df_final['subsistema'] = 'NE'\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. FEATURES DERIVADAS\n",
    "# ==============================================================================\n",
    "print(\"\\n6. Calculando features derivadas...\")\n",
    "\n",
    "# Corte eolico (curtailment) - quando capacidade > geracao\n",
    "# Em teoria, a capacidade maxima de geracao (assumindo 100% de CF) seria capacidade_mw\n",
    "# Na pratica, com vento bom, um CF de ~50% e alto. Vamos usar uma estimativa conservadora:\n",
    "# Potencial maximo mensal (horas no mes * capacidade * fator de disponibilidade ~95%)\n",
    "\n",
    "# Horas no mes\n",
    "df_final['dias_no_mes'] = df_final.apply(\n",
    "    lambda row: pd.Timestamp(year=int(row['ano']), month=int(row['mes']), day=1).days_in_month,\n",
    "    axis=1\n",
    ")\n",
    "df_final['horas_no_mes'] = df_final['dias_no_mes'] * 24\n",
    "\n",
    "# Potencial maximo teorico (MWh) = capacidade_mw * horas * 0.95 (disponibilidade)\n",
    "df_final['potencial_max_mwh'] = df_final['capacidade_eolica_ne_mw'] * df_final['horas_no_mes'] * 0.95\n",
    "\n",
    "# Geracao efetiva (MWh) = geracao_mwmed * horas\n",
    "df_final['geracao_efetiva_mwh'] = df_final['geracao_eolica_ne_mwmed'] * df_final['horas_no_mes']\n",
    "\n",
    "# Corte eolico = potencial - efetivo (quando positivo)\n",
    "# Na verdade, o corte e mais complexo, mas uma proxy e:\n",
    "# Se capacity_factor muito baixo quando vento e bom = corte\n",
    "# Vamos usar uma abordagem simples: assumir que CF > 40% e raro sem corte\n",
    "# Corte estimado (MWmed) = max(0, capacidade * 0.40 - geracao)\n",
    "# Mas isso e simplificacao. Melhor e deixar o modelo aprender.\n",
    "\n",
    "# Por enquanto, vamos calcular o \"corte potencial\" como:\n",
    "# Se geracao < capacidade * 0.30 (muito baixo, provavelmente sem vento, nao e corte)\n",
    "# Se geracao entre 30-50% da capacidade, pode ser vento normal\n",
    "# Se geracao > 50%, improvavel ter corte significativo\n",
    "# O corte real viria de dados de restrições operativas do ONS que nao temos\n",
    "\n",
    "# Vamos fazer uma estimativa conservadora:\n",
    "# corte_estimado_mwmed = max(0, (capacidade * 0.45) - geracao)\n",
    "# Onde 0.45 e um CF \"alvo\" razoavel para NE\n",
    "\n",
    "df_final['corte_eolica_ne_mwmed'] = np.maximum(\n",
    "    0,\n",
    "    (df_final['capacidade_eolica_ne_mw'] * 0.45) - df_final['geracao_eolica_ne_mwmed']\n",
    ")\n",
    "\n",
    "# Capacity Factor (%)\n",
    "df_final['capacity_factor_ne'] = (\n",
    "    df_final['geracao_eolica_ne_mwmed'] / df_final['capacidade_eolica_ne_mw'].replace(0, np.nan)\n",
    ") * 100\n",
    "\n",
    "# Penetracao eolica no NE (%)\n",
    "df_final['penetracao_eolica_ne'] = (\n",
    "    df_final['geracao_eolica_ne_mwmed'] / df_final['carga_ne_mwmed'].replace(0, np.nan)\n",
    ") * 100\n",
    "\n",
    "# Penetracao eolica no SIN (%)\n",
    "df_final['penetracao_eolica_sin'] = (\n",
    "    df_final['geracao_eolica_ne_mwmed'] / df_final['demanda_sin_mwmed'].replace(0, np.nan)\n",
    ") * 100\n",
    "\n",
    "# Crescimento de capacidade (MW adicionados no mes)\n",
    "df_final = df_final.sort_values(['ano', 'mes']).reset_index(drop=True)\n",
    "df_final['crescimento_capacidade_mw'] = df_final['capacidade_eolica_ne_mw'].diff().fillna(0)\n",
    "\n",
    "# Indice sazonal (mes do ano)\n",
    "df_final['mes_sazonal'] = df_final['mes']\n",
    "\n",
    "# Trimestre\n",
    "df_final['trimestre'] = ((df_final['mes'] - 1) // 3) + 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. ORDENAR E SELECIONAR COLUNAS FINAIS\n",
    "# ==============================================================================\n",
    "colunas_finais = [\n",
    "    'ano', 'mes', 'subsistema',\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'capacidade_eolica_ne_mw',\n",
    "    'carga_ne_mwmed',\n",
    "    'demanda_sin_mwmed',\n",
    "    'demanda_max_sin_mw',\n",
    "    'corte_eolica_ne_mwmed',\n",
    "    'capacity_factor_ne',\n",
    "    'penetracao_eolica_ne',\n",
    "    'penetracao_eolica_sin',\n",
    "    'crescimento_capacidade_mw',\n",
    "    'trimestre'\n",
    "]\n",
    "\n",
    "df_final = df_final[colunas_finais]\n",
    "df_final = df_final.sort_values(['ano', 'mes']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset final: {df_final.shape}\")\n",
    "print(f\"\\nPrimeiras 10 linhas:\")\n",
    "print(df_final.head(10).to_string())\n",
    "print(f\"\\nUltimas 10 linhas:\")\n",
    "print(df_final.tail(10).to_string())\n",
    "\n",
    "print(f\"\\nEstatisticas:\")\n",
    "print(df_final.describe())\n",
    "\n",
    "print(f\"\\nValores nulos por coluna:\")\n",
    "print(df_final.isnull().sum())\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. SALVAR\n",
    "# ==============================================================================\n",
    "output_file = os.path.join(pasta_dados, \"dados_operacao_ONS.csv\")\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"ARQUIVO SALVO: {output_file}\")\n",
    "print(f\"Total de registros: {len(df_final)}\")\n",
    "print(f\"Periodo: {df_final['ano'].min()}-{df_final['mes'].min():02d} ate {df_final['ano'].max()}-{df_final['mes'].max():02d}\")\n",
    "print(f\"\\n✅ CORREÇÃO APLICADA:\")\n",
    "print(f\"  - Capacidade agora considera entrada E desativação de usinas\")\n",
    "print(f\"  - Apenas usinas ATIVAS em cada mês são contabilizadas\")\n",
    "print(f\"\\nColunas criadas:\")\n",
    "for col in colunas_finais:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fcf084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PROCESSAMENTO ERA5 - EXTRACAO DE DADOS POR PARQUE\n",
      "====================================================================================================\n",
      "\n",
      "1. Carregando mapeamento parques -> células...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total de parques: 2422\n",
      "   Cell_keys únicos: 225\n",
      "   Período de processamento: 2015 a 2025\n",
      "\n",
      "1.1. Adicionando datas de operação dos parques...\n",
      "   CEGs com datas de operação: 946\n",
      "\n",
      "   Parques com data de entrada: 927\n",
      "   Parques com data de desativação: 0\n",
      "\n",
      "2. Verificando arquivos disponíveis...\n",
      "   Arquivos encontrados: 2475\n",
      "   Formato detectado: ZIP (com NetCDFs)\n",
      "   Células únicas com dados: 225\n",
      "\n",
      "3. Extraindo dados meteorológicos por parque...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando células: 100%|██████████| 225/225 [05:04<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Criando DataFrame final...\n",
      "   Total de registros extraídos: 319704\n",
      "   Parques únicos: 2403\n",
      "   Período: 2015/01 a 2025/12\n",
      "\n",
      "5. Verificando qualidade dos dados...\n",
      "\n",
      "   Valores nulos por coluna:\n",
      "dat_entradaoperacao    197340\n",
      "dat_desativacao        319704\n",
      "sst                    279840\n",
      "dtype: int64\n",
      "\n",
      "   Estatísticas das principais variáveis:\n",
      "       vel_vento_100m  vel_vento_10m            t2m             sp  \\\n",
      "count   319704.000000  319704.000000  319704.000000  319704.000000   \n",
      "mean         6.035771       4.150617     298.832619   96342.897118   \n",
      "std          1.864053       1.590562       1.955270    3023.935327   \n",
      "min          0.713009       0.409755     290.461090   89570.250000   \n",
      "25%          4.772943       3.046142     297.716949   93923.156250   \n",
      "50%          5.983365       3.936842     299.144592   95763.656250   \n",
      "75%          7.376560       5.175521     300.240234   99201.437500   \n",
      "max         11.903348       9.911392     304.864441  101538.898438   \n",
      "\n",
      "        densidade_ar  peso_capacidade_mw  \n",
      "count  319704.000000       319704.000000  \n",
      "mean        1.123034           25.623808  \n",
      "std         0.031904           19.765080  \n",
      "min         1.048865            0.000000  \n",
      "25%         1.098247            0.050400  \n",
      "50%         1.115333           27.300000  \n",
      "75%         1.153300           38.500000  \n",
      "max         1.189545          105.000000  \n",
      "\n",
      "6. Salvando dados processados...\n",
      "\n",
      "====================================================================================================\n",
      "DADOS ERA5 PROCESSADOS COM SUCESSO!\n",
      "====================================================================================================\n",
      "\n",
      "Resumo:\n",
      "  - Arquivo salvo: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\dados_era5_parques_ne.csv\n",
      "  - Total de registros: 319704\n",
      "  - Parques únicos: 2403\n",
      "  - Período: 2015/01 a 2025/12\n",
      "\n",
      "Colunas do dataset:\n",
      "   1. ano\n",
      "   2. mes\n",
      "   3. nome_parque\n",
      "   4. CEG_norm\n",
      "   5. lat\n",
      "   6. lon\n",
      "   7. lat_era5\n",
      "   8. lon_era5\n",
      "   9. cell_key\n",
      "  10. peso_capacidade_mw\n",
      "  11. dat_entradaoperacao\n",
      "  12. dat_desativacao\n",
      "  13. uf\n",
      "  14. u10\n",
      "  15. v10\n",
      "  16. u100\n",
      "  17. v100\n",
      "  18. vel_vento_10m\n",
      "  19. vel_vento_100m\n",
      "  20. t2m\n",
      "  21. sp\n",
      "  22. tp\n",
      "  23. sst\n",
      "  24. zust\n",
      "  25. densidade_ar\n",
      "  26. wind_shear\n",
      "\n",
      "Primeiras 5 linhas:\n",
      "    ano  mes       nome_parque            CEG_norm       lat        lon  lat_era5  lon_era5            cell_key  peso_capacidade_mw dat_entradaoperacao dat_desativacao  uf       u10       v10      u100      v100  vel_vento_10m  vel_vento_100m         t2m            sp        tp  sst      zust  densidade_ar  wind_shear\n",
      "0  2015    1       AW Cruzeiro  EOLCVRN032201-6-01 -5.363008 -36.151113     -5.25    -36.25  latm5p25_lonm36p25                37.8                 NaT             NaT  RN -6.089840  1.628532 -7.849039  2.338771       6.303830        8.190071  300.879883  100742.78125  0.000508  NaN  0.366831      1.166410    0.113683\n",
      "1  2015    1   AW Nova Arizona  EOLCVRN032196-6-01 -5.350713 -36.143941     -5.25    -36.25  latm5p25_lonm36p25                21.0                 NaT             NaT  RN -6.089840  1.628532 -7.849039  2.338771       6.303830        8.190071  300.879883  100742.78125  0.000508  NaN  0.366831      1.166410    0.113683\n",
      "2  2015    1  AW Olho D’Água I  EOLCVRN032199-0-01 -5.396052 -36.140033     -5.50    -36.25  latm5p50_lonm36p25                25.2                 NaT             NaT  RN -5.106383  2.025563 -7.232078  3.014614       5.493456        7.835231  301.121246   99839.31250  0.000506  NaN  0.395705      1.155023    0.154206\n",
      "3  2015    1    AW Santa Régia  EOLCVRN032194-0-01 -5.386859 -36.110488     -5.50    -36.00  latm5p50_lonm36p00                37.8          2023-09-13             NaT  RN -4.438643  2.019114 -6.536033  3.107558       4.876308        7.237171  300.573242  100028.28125  0.000675  NaN  0.343871      1.159319    0.171478\n",
      "4  2015    1       AW São João  EOLCVRN032080-3-01 -5.366268 -36.110468     -5.25    -36.00  latm5p25_lonm36p00                25.2          2023-07-21             NaT  RN -5.895033  2.376176 -7.661082  3.242654       6.355913        8.319074  300.597717  100701.28125  0.000690  NaN  0.360286      1.167024    0.116897\n",
      "\n",
      "Próximo passo: Executar célula de Agregação ERA5\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Processamento ERA5 - Extração de Dados por Parque\n",
    "## Correções aplicadas:\n",
    "## 1. Arquivos .nc são ZIPs - agora descompacta automaticamente\n",
    "## 2. Coordenada temporal é 'valid_time', não 'time'\n",
    "## 3. Combina os 3 datasets (avgua, avgad, avgid) normalizando timestamps\n",
    "## 4. Timestamps normalizados para apenas data (sem hora)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"PROCESSAMENTO ERA5 - EXTRACAO DE DADOS POR PARQUE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNCAO AUXILIAR: Abrir arquivo ERA5 (ZIP ou NetCDF)\n",
    "# ==============================================================================\n",
    "def abrir_era5_arquivo(arquivo_path):\n",
    "    \"\"\"\n",
    "    Abre arquivo ERA5 que pode ser ZIP (com 3 NetCDFs) ou NetCDF direto.\n",
    "    Normaliza timestamps para apenas data (remove hora) antes de combinar.\n",
    "    Retorna um xarray Dataset combinado com todas as variáveis.\n",
    "    \"\"\"\n",
    "    with open(arquivo_path, 'rb') as f:\n",
    "        header = f.read(4)\n",
    "\n",
    "    if header[:2] == b'PK':  # É um arquivo ZIP\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            # Extrair todos os arquivos do ZIP\n",
    "            with zipfile.ZipFile(arquivo_path, 'r') as z:\n",
    "                z.extractall(temp_dir)\n",
    "\n",
    "            datasets = []\n",
    "            for nc_file in glob(os.path.join(temp_dir, '*.nc')):\n",
    "                ds = xr.open_dataset(nc_file, engine='netcdf4')\n",
    "\n",
    "                # Normalizar timestamps para apenas data (primeiro dia do mês, meia-noite)\n",
    "                if 'valid_time' in ds.coords:\n",
    "                    # Converter para pandas datetime e normalizar\n",
    "                    new_times = pd.to_datetime(ds['valid_time'].values).normalize()\n",
    "                    ds = ds.assign_coords(valid_time=new_times)\n",
    "\n",
    "                datasets.append(ds)\n",
    "\n",
    "            if not datasets:\n",
    "                raise ValueError(f\"Nenhum NetCDF encontrado no ZIP: {arquivo_path}\")\n",
    "\n",
    "            # Combinar datasets (agora com timestamps alinhados)\n",
    "            ds_combined = xr.merge(datasets, compat='override', join='exact')\n",
    "\n",
    "            # Carregar em memória\n",
    "            ds_memory = ds_combined.load()\n",
    "\n",
    "            # Fechar datasets originais\n",
    "            for ds in datasets:\n",
    "                ds.close()\n",
    "            ds_combined.close()\n",
    "\n",
    "            return ds_memory\n",
    "\n",
    "        finally:\n",
    "            # Limpar diretório temporário\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    else:\n",
    "        # É NetCDF direto\n",
    "        return xr.open_dataset(arquivo_path, engine='netcdf4')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAR MAPEAMENTO PARQUES -> CELLS E ADICIONAR DATAS DE OPERAÇÃO\n",
    "# ==============================================================================\n",
    "print(\"\\n1. Carregando mapeamento parques -> células...\")\n",
    "\n",
    "mapeamento_file = os.path.join(pasta_dados, 'mapeamento_parques_cells.csv')\n",
    "\n",
    "if not os.path.exists(mapeamento_file):\n",
    "    print(f\"   ERRO: Arquivo de mapeamento não encontrado: {mapeamento_file}\")\n",
    "    print(f\"   Execute primeiro a célula de Coleta ERA5!\")\n",
    "    raise FileNotFoundError(mapeamento_file)\n",
    "\n",
    "df_parques = pd.read_csv(mapeamento_file)\n",
    "\n",
    "print(f\"   Total de parques: {len(df_parques)}\")\n",
    "print(f\"   Cell_keys únicos: {df_parques['cell_key'].nunique()}\")\n",
    "print(f\"   Período de processamento: {min(anos)} a {max(anos)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1.1. ADICIONAR DATAS DE OPERAÇÃO DO CAPACIDADE_GERACAO.CSV\n",
    "# ==============================================================================\n",
    "print(\"\\n1.1. Adicionando datas de operação dos parques...\")\n",
    "\n",
    "# Normalizar CEG\n",
    "def normalizar_ceg(ceg):\n",
    "    \"\"\"Normaliza CEG para buscar em CAPACIDADE_GERACAO\"\"\"\n",
    "    if pd.isna(ceg):\n",
    "        return None\n",
    "    s = str(ceg).strip().upper()\n",
    "    if s in {\"\", \"-\"}:\n",
    "        return None\n",
    "\n",
    "    last_dot_idx = s.rfind('.')\n",
    "    if last_dot_idx != -1:\n",
    "        s = s[:last_dot_idx] + '-' + s[last_dot_idx+1:]\n",
    "\n",
    "    s = s.replace(\".\", \"\")\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s\n",
    "\n",
    "# Carregar CAPACIDADE_GERACAO.csv\n",
    "arquivo_cap = os.path.join(pot_ons, \"CAPACIDADE_GERACAO.csv\")\n",
    "df_cap = pd.read_csv(arquivo_cap, sep=';')\n",
    "\n",
    "# Filtrar apenas eólicas no NE\n",
    "df_cap_eol_ne = df_cap[\n",
    "    (df_cap['nom_tipousina'].str.upper().str.contains('EOLI', na=False)) &\n",
    "    (df_cap['nom_subsistema'].str.upper().str.contains('NORDESTE', na=False))\n",
    "].copy()\n",
    "\n",
    "# Normalizar CEG\n",
    "df_cap_eol_ne['ceg_norm'] = df_cap_eol_ne['ceg'].apply(normalizar_ceg)\n",
    "\n",
    "# Converter datas\n",
    "df_cap_eol_ne['dat_entradaoperacao'] = pd.to_datetime(df_cap_eol_ne['dat_entradaoperacao'], errors='coerce')\n",
    "df_cap_eol_ne['dat_desativacao'] = pd.to_datetime(df_cap_eol_ne['dat_desativacao'], errors='coerce')\n",
    "\n",
    "# Para cada CEG, pegar a data MÍNIMA de entrada e MÁXIMA de desativação\n",
    "datas_por_ceg = df_cap_eol_ne.groupby('ceg_norm').agg({\n",
    "    'dat_entradaoperacao': 'min',\n",
    "    'dat_desativacao': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"   CEGs com datas de operação: {len(datas_por_ceg)}\")\n",
    "\n",
    "# Merge com df_parques\n",
    "df_parques = df_parques.merge(datas_por_ceg, left_on='CEG_norm', right_on='ceg_norm', how='left')\n",
    "\n",
    "print(f\"\\n   Parques com data de entrada: {df_parques['dat_entradaoperacao'].notna().sum()}\")\n",
    "print(f\"   Parques com data de desativação: {df_parques['dat_desativacao'].notna().sum()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. VERIFICAR ARQUIVOS DISPONÍVEIS\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Verificando arquivos disponíveis...\")\n",
    "\n",
    "# Padrão: Dados/ERA5_Bruto/ano=YYYY/Era5M_NE-{cell_key}_{ano}.nc\n",
    "arquivos_nc = []\n",
    "for ano in anos:\n",
    "    ano_dir = os.path.join(pasta_era5, f\"ano={ano}\")\n",
    "    if os.path.exists(ano_dir):\n",
    "        arquivos = glob(os.path.join(ano_dir, \"Era5M_NE-*.nc\"))\n",
    "        arquivos_nc.extend(arquivos)\n",
    "\n",
    "print(f\"   Arquivos encontrados: {len(arquivos_nc)}\")\n",
    "\n",
    "if len(arquivos_nc) == 0:\n",
    "    print(f\"\\n   AVISO: Nenhum arquivo encontrado em {pasta_era5}/ano=YYYY/\")\n",
    "    print(f\"   Execute primeiro a célula de Coleta ERA5!\")\n",
    "    raise FileNotFoundError(\"Nenhum arquivo encontrado\")\n",
    "\n",
    "# Verificar formato (ZIP ou NetCDF)\n",
    "with open(arquivos_nc[0], 'rb') as f:\n",
    "    header = f.read(4)\n",
    "formato = \"ZIP (com NetCDFs)\" if header[:2] == b'PK' else \"NetCDF direto\"\n",
    "print(f\"   Formato detectado: {formato}\")\n",
    "\n",
    "# Criar dicionário: cell_key -> {ano: arquivo}\n",
    "arquivos_por_cell = {}\n",
    "for arquivo in arquivos_nc:\n",
    "    nome = os.path.basename(arquivo)\n",
    "    # Formato: Era5M_NE-latm10p00_lonm38p75_2015.nc\n",
    "    partes = nome.replace('Era5M_NE-', '').replace('.nc', '').split('_')\n",
    "\n",
    "    # cell_key = latm10p00_lonm38p75\n",
    "    cell_key = '_'.join(partes[:-1])\n",
    "    ano = int(partes[-1])\n",
    "\n",
    "    if cell_key not in arquivos_por_cell:\n",
    "        arquivos_por_cell[cell_key] = {}\n",
    "    arquivos_por_cell[cell_key][ano] = arquivo\n",
    "\n",
    "print(f\"   Células únicas com dados: {len(arquivos_por_cell)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESSAR ARQUIVOS E EXTRAIR DADOS POR PARQUE\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Extraindo dados meteorológicos por parque...\")\n",
    "\n",
    "dados_extraidos = []\n",
    "erros_processamento = []\n",
    "\n",
    "# Agrupar parques por cell_key para processar por célula\n",
    "parques_por_cell = df_parques.groupby('cell_key')\n",
    "\n",
    "for cell_key, grupo_parques in tqdm(parques_por_cell, desc=\"Processando células\"):\n",
    "\n",
    "    if cell_key not in arquivos_por_cell:\n",
    "        continue\n",
    "\n",
    "    arquivos_ano = arquivos_por_cell[cell_key]\n",
    "\n",
    "    # Para cada ano\n",
    "    for ano in anos:\n",
    "        if ano not in arquivos_ano:\n",
    "            continue\n",
    "\n",
    "        arquivo_nc = arquivos_ano[ano]\n",
    "\n",
    "        try:\n",
    "            # Abrir arquivo (ZIP ou NetCDF)\n",
    "            ds = abrir_era5_arquivo(arquivo_nc)\n",
    "\n",
    "            # Detectar nome da coordenada temporal\n",
    "            time_coord = None\n",
    "            for possible_name in ['valid_time', 'time', 'month']:\n",
    "                if possible_name in ds.coords:\n",
    "                    time_coord = possible_name\n",
    "                    break\n",
    "\n",
    "            if time_coord is None:\n",
    "                raise ValueError(f\"Coordenada temporal não encontrada. Coords: {list(ds.coords)}\")\n",
    "\n",
    "            # Para cada parque nesta célula\n",
    "            for _, parque in grupo_parques.iterrows():\n",
    "\n",
    "                # Coordenadas ERA5 da célula\n",
    "                lat_era5 = parque['lat_era5']\n",
    "                lon_era5 = parque['lon_era5']\n",
    "\n",
    "                # Selecionar ponto mais próximo no grid\n",
    "                ponto = ds.sel(latitude=lat_era5, longitude=lon_era5, method='nearest')\n",
    "\n",
    "                # Para cada mês (time dimension)\n",
    "                for t_idx, tempo in enumerate(ponto[time_coord].values):\n",
    "\n",
    "                    # Extrair timestamp\n",
    "                    timestamp = pd.Timestamp(tempo)\n",
    "                    mes = timestamp.month\n",
    "                    ano_ref = timestamp.year\n",
    "\n",
    "                    # Selecionar dados do mês\n",
    "                    dados_mes = ponto.isel({time_coord: t_idx})\n",
    "\n",
    "                    # Extrair componentes de vento\n",
    "                    u10 = float(dados_mes['u10'].values) if 'u10' in dados_mes else np.nan\n",
    "                    v10 = float(dados_mes['v10'].values) if 'v10' in dados_mes else np.nan\n",
    "                    u100 = float(dados_mes['u100'].values) if 'u100' in dados_mes else np.nan\n",
    "                    v100 = float(dados_mes['v100'].values) if 'v100' in dados_mes else np.nan\n",
    "\n",
    "                    # Calcular velocidade do vento\n",
    "                    vel_vento_10m = np.sqrt(u10**2 + v10**2) if not (np.isnan(u10) or np.isnan(v10)) else np.nan\n",
    "                    vel_vento_100m = np.sqrt(u100**2 + v100**2) if not (np.isnan(u100) or np.isnan(v100)) else np.nan\n",
    "\n",
    "                    # Outras variáveis\n",
    "                    t2m = float(dados_mes['t2m'].values) if 't2m' in dados_mes else np.nan\n",
    "                    sp = float(dados_mes['sp'].values) if 'sp' in dados_mes else np.nan\n",
    "                    tp = float(dados_mes['tp'].values) if 'tp' in dados_mes else np.nan\n",
    "                    sst = float(dados_mes['sst'].values) if 'sst' in dados_mes else np.nan\n",
    "                    zust = float(dados_mes['zust'].values) if 'zust' in dados_mes else np.nan\n",
    "\n",
    "                    # Densidade do ar (rho = P / (R * T))\n",
    "                    R = 287.058  # J/(kg*K) - constante específica do ar\n",
    "                    densidade_ar = sp / (R * t2m) if not (np.isnan(sp) or np.isnan(t2m) or t2m == 0) else np.nan\n",
    "\n",
    "                    # Wind shear (proxy simples: ln(v100/v10) / ln(100/10))\n",
    "                    if vel_vento_100m > 0 and vel_vento_10m > 0:\n",
    "                        wind_shear = np.log(vel_vento_100m / vel_vento_10m) / np.log(100.0 / 10.0)\n",
    "                    else:\n",
    "                        wind_shear = np.nan\n",
    "\n",
    "                    # Armazenar dados do parque\n",
    "                    dados_extraidos.append({\n",
    "                        'ano': ano_ref,\n",
    "                        'mes': mes,\n",
    "                        'nome_parque': parque['nome_parque'],\n",
    "                        'CEG_norm': parque['CEG_norm'],\n",
    "                        'lat': parque['latitude_parque'],\n",
    "                        'lon': parque['longitude_parque'],\n",
    "                        'lat_era5': lat_era5,\n",
    "                        'lon_era5': lon_era5,\n",
    "                        'cell_key': cell_key,\n",
    "                        'peso_capacidade_mw': parque['capacidade_mw'],  # PESO para agregação!\n",
    "                        'dat_entradaoperacao': parque['dat_entradaoperacao'],\n",
    "                        'dat_desativacao': parque['dat_desativacao'],\n",
    "                        'uf': parque['uf'],\n",
    "                        'u10': u10,\n",
    "                        'v10': v10,\n",
    "                        'u100': u100,\n",
    "                        'v100': v100,\n",
    "                        'vel_vento_10m': vel_vento_10m,\n",
    "                        'vel_vento_100m': vel_vento_100m,\n",
    "                        't2m': t2m,\n",
    "                        'sp': sp,\n",
    "                        'tp': tp,\n",
    "                        'sst': sst,\n",
    "                        'zust': zust,\n",
    "                        'densidade_ar': densidade_ar,\n",
    "                        'wind_shear': wind_shear,\n",
    "                    })\n",
    "\n",
    "            ds.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            erros_processamento.append({\n",
    "                'cell_key': cell_key,\n",
    "                'ano': ano,\n",
    "                'arquivo': arquivo_nc,\n",
    "                'erro': str(e)\n",
    "            })\n",
    "            print(f\"\\n   ERRO ao processar {cell_key} - {ano}: {e}\")\n",
    "            continue\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CRIAR DATAFRAME FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n4. Criando DataFrame final...\")\n",
    "\n",
    "df_era5_parques = pd.DataFrame(dados_extraidos)\n",
    "\n",
    "if len(df_era5_parques) == 0:\n",
    "    print(f\"\\n   ERRO: Nenhum dado foi extraído!\")\n",
    "    if erros_processamento:\n",
    "        print(f\"\\n   Erros de processamento:\")\n",
    "        df_erros = pd.DataFrame(erros_processamento)\n",
    "        print(df_erros.to_string())\n",
    "    raise ValueError(\"Nenhum dado extraído dos arquivos\")\n",
    "\n",
    "print(f\"   Total de registros extraídos: {len(df_era5_parques)}\")\n",
    "print(f\"   Parques únicos: {df_era5_parques['nome_parque'].nunique()}\")\n",
    "print(f\"   Período: {df_era5_parques['ano'].min()}/{df_era5_parques['mes'].min():02d} a {df_era5_parques['ano'].max()}/{df_era5_parques['mes'].max():02d}\")\n",
    "\n",
    "# Ordenar por ano, mês, parque\n",
    "df_era5_parques = df_era5_parques.sort_values(['ano', 'mes', 'nome_parque']).reset_index(drop=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. VERIFICAR QUALIDADE DOS DADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n5. Verificando qualidade dos dados...\")\n",
    "\n",
    "print(f\"\\n   Valores nulos por coluna:\")\n",
    "nulos = df_era5_parques.isnull().sum()\n",
    "if nulos.sum() > 0:\n",
    "    print(nulos[nulos > 0])\n",
    "else:\n",
    "    print(\"   Nenhum valor nulo!\")\n",
    "\n",
    "print(f\"\\n   Estatísticas das principais variáveis:\")\n",
    "cols_stats = ['vel_vento_100m', 'vel_vento_10m', 't2m', 'sp', 'densidade_ar', 'peso_capacidade_mw']\n",
    "print(df_era5_parques[cols_stats].describe())\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SALVAR DADOS PROCESSADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n6. Salvando dados processados...\")\n",
    "\n",
    "output_file = os.path.join(pasta_dados, 'dados_era5_parques_ne.csv')\n",
    "df_era5_parques.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"DADOS ERA5 PROCESSADOS COM SUCESSO!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nResumo:\")\n",
    "print(f\"  - Arquivo salvo: {output_file}\")\n",
    "print(f\"  - Total de registros: {len(df_era5_parques)}\")\n",
    "print(f\"  - Parques únicos: {df_era5_parques['nome_parque'].nunique()}\")\n",
    "print(f\"  - Período: {df_era5_parques['ano'].min()}/{df_era5_parques['mes'].min():02d} a {df_era5_parques['ano'].max()}/{df_era5_parques['mes'].max():02d}\")\n",
    "\n",
    "print(f\"\\nColunas do dataset:\")\n",
    "for i, col in enumerate(df_era5_parques.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "print(df_era5_parques.head().to_string())\n",
    "\n",
    "if erros_processamento:\n",
    "    print(f\"\\n   AVISOS: {len(erros_processamento)} arquivos com erro:\")\n",
    "    df_erros = pd.DataFrame(erros_processamento)\n",
    "    print(df_erros.head(10).to_string())\n",
    "\n",
    "    # Salvar erros\n",
    "    error_file = os.path.join(pasta_dados, 'erros_processamento_era5.csv')\n",
    "    df_erros.to_csv(error_file, index=False)\n",
    "    print(f\"\\n   Erros salvos em: {error_file}\")\n",
    "\n",
    "print(f\"\\nPróximo passo: Executar célula de Agregação ERA5\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e37ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "AGREGACAO ERA5 - MEDIA PONDERADA POR CAPACIDADE INSTALADA (DINAMICA)\n",
      "====================================================================================================\n",
      "\n",
      "1. Carregando dados ERA5 por parque...\n",
      "   Total de registros: 319704\n",
      "   Parques únicos: 2403\n",
      "   Período: 2015/01 a 2025/12\n",
      "\n",
      "2. Aplicando filtro temporal (parques ativos por mês)...\n",
      "   Registros ANTES do filtro: 319704\n",
      "   Registros DEPOIS do filtro (ativos): 66747\n",
      "   Registros removidos (inativos): 252957\n",
      "   % removido: 79.1%\n",
      "\n",
      "3. Calculando médias ponderadas por mês (apenas parques ativos)...\n",
      "\n",
      "   Total de meses processados: 132\n",
      "\n",
      "   Primeiras 10 linhas:\n",
      "    ano  mes  vel_vento_10m_ne_media  vel_vento_100m_ne_media  t2m_ne_media   sp_ne_media  tp_ne_media  sst_ne_media  zust_ne_media  densidade_ar_ne_media  wind_shear_ne_media  capacidade_total_ne_mw  num_parques_ativos\n",
      "0  2015    1                5.948146                 7.639418    299.799070  99009.233073     0.000755    300.929169       0.301586               1.150351             0.121188               3409.4252               117.0\n",
      "1  2015    2                5.094896                 6.525404    299.645129  98944.133969     0.003148    301.340341       0.262136               1.150158             0.120576               3409.4252               117.0\n",
      "2  2015    3                4.424697                 5.735729    299.680798  98900.134898     0.007106    301.609785       0.225851               1.149585             0.123057               3583.3252               124.0\n",
      "3  2015    4                3.943846                 5.136281    299.602167  98779.245266     0.004936    301.987625       0.213489               1.148442             0.125477               3760.9252               130.0\n",
      "4  2015    5                5.631810                 7.323204    299.369173  98906.001496     0.001869    301.825497       0.287662               1.150691             0.125252               3819.7852               132.0\n",
      "5  2015    6                5.839348                 7.638977    298.531223  99166.408151     0.001599    301.129716       0.303061               1.156965             0.125918               3981.7852               138.0\n",
      "6  2015    7                6.347425                 8.363413    297.975341  99197.621808     0.003013    300.348291       0.325532               1.159506             0.129907               4011.4852               139.0\n",
      "7  2015    8                7.027859                 9.329388    298.156355  98979.735197     0.000234    299.776749       0.365947               1.156277             0.133555               4186.9852               145.0\n",
      "8  2015    9                6.749404                 8.837998    299.355372  98879.148120     0.000261    299.972282       0.347550               1.150622             0.130287               4270.5852               148.0\n",
      "9  2015   10                6.762142                 8.840139    299.818197  98777.389783     0.000226    300.165592       0.357821               1.147700             0.127860               4376.2852               153.0\n",
      "\n",
      "4. Calculando percentis de vento...\n",
      "   Percentis calculados para 132 meses\n",
      "\n",
      "5. Calculando features derivadas...\n",
      "   Features derivadas calculadas\n",
      "\n",
      "6. Preparando dataset final...\n",
      "\n",
      "   Dataset final: (132, 21)\n",
      "\n",
      "   Primeiras 10 linhas:\n",
      "    ano  mes  vel_vento_100m_ne_media  vel_vento_10m_ne_media  t2m_ne_media  temp_celsius_ne   sp_ne_media  sst_ne_media  zust_ne_media  densidade_ar_ne_media  wind_shear_ne_media  vel_vento_100m_ne_p10  vel_vento_100m_ne_p90  vel_vento_10m_ne_p10  vel_vento_10m_ne_p90  potencia_vento_100m_ne  precipitacao_mensal_mm  ratio_vento_100_10  variabilidade_vento_100m  capacidade_total_ne_mw  num_parques_ativos\n",
      "0  2015    1                 7.639418                5.948146    299.799070        26.649070  99009.233073    300.929169       0.301586               1.150351             0.121188               5.295773               9.053837              3.219683              7.706232              256.437274                0.754608            1.284336                  3.758063               3409.4252               117.0\n",
      "1  2015    2                 6.525404                5.094896    299.645129        26.495129  98944.133969    301.340341       0.262136               1.150158             0.120576               4.410039               7.795771              2.664193              6.698663              159.790057                3.148107            1.280773                  3.385732               3409.4252               117.0\n",
      "2  2015    3                 5.735729                4.424697    299.680798        26.530798  98900.134898    301.609785       0.225851               1.149585             0.123057               4.452263               7.004239              2.710917              6.090802              108.461853                7.106010            1.296299                  2.551976               3583.3252               124.0\n",
      "3  2015    4                 5.136281                3.943846    299.602167        26.452167  98779.245266    301.987625       0.213489               1.148442             0.125477               4.268426               6.161692              2.526000              5.397149               77.808221                4.936248            1.302353                  1.893266               3760.9252               130.0\n",
      "4  2015    5                 7.323204                5.631810    299.369173        26.219173  98906.001496    301.825497       0.287662               1.150691             0.125252               5.437303               8.700244              3.253444              7.509815              225.960293                1.868970            1.300329                  3.262942               3819.7852               132.0\n",
      "5  2015    6                 7.638977                5.839348    298.531223        25.381223  99166.408151    301.129716       0.303061               1.156965             0.125918               6.261766               8.643447              3.799428              7.405810              257.867055                1.598501            1.308190                  2.381680               3981.7852               138.0\n",
      "6  2015    7                 8.363413                6.347425    297.975341        24.825341  99197.621808    300.348291       0.325532               1.159506             0.129907               6.524470               9.656052              3.988320              8.152605              339.151501                3.013227            1.317607                  3.131582               4011.4852               139.0\n",
      "7  2015    8                 9.329388                7.027859    298.156355        25.006355  98979.735197    299.776749       0.365947               1.156277             0.133555               6.360735              10.858037              3.915312              9.066014              469.452179                0.234400            1.327487                  4.497302               4186.9852               145.0\n",
      "8  2015    9                 8.837998                6.749404    299.355372        26.205372  98879.148120    299.972282       0.347550               1.150622             0.130287               5.928469              10.699225              3.632141              9.022312              397.159015                0.260588            1.309449                  4.770756               4270.5852               148.0\n",
      "9  2015   10                 8.840139                6.762142    299.818197        26.668197  98777.389783    300.165592       0.357821               1.147700             0.127860               6.210974              10.531735              3.885205              8.858328              396.438513                0.226485            1.307299                  4.320761               4376.2852               153.0\n",
      "\n",
      "   Últimas 10 linhas:\n",
      "      ano  mes  vel_vento_100m_ne_media  vel_vento_10m_ne_media  t2m_ne_media  temp_celsius_ne   sp_ne_media  sst_ne_media  zust_ne_media  densidade_ar_ne_media  wind_shear_ne_media  vel_vento_100m_ne_p10  vel_vento_100m_ne_p90  vel_vento_10m_ne_p10  vel_vento_10m_ne_p90  potencia_vento_100m_ne  precipitacao_mensal_mm  ratio_vento_100_10  variabilidade_vento_100m  capacidade_total_ne_mw  num_parques_ativos\n",
      "122  2025    3                 5.489754                3.792552    299.774294        26.624294  96528.627421    302.020345       0.242298               1.121675             0.166186               4.636717               6.557450              3.036234              5.108580               92.788807                2.392723            1.447509                  1.920733              28938.1752               907.0\n",
      "123  2025    4                 4.951267                3.394907    299.979542        26.829542  96448.539920    302.354479       0.221439               1.119946             0.173345               3.990782               6.283071              2.520100              4.916401               67.969801                1.834691            1.458440                  2.292290              29136.1752               910.0\n",
      "124  2025    5                 6.925010                4.785392    298.462238        25.312238  96690.489115    301.970502       0.305425               1.128381             0.165780               6.020515               7.897480              3.827462              6.266639              187.364350                1.302346            1.447114                  1.876965              29163.1752               911.0\n",
      "125  2025    6                 7.039812                4.857420    297.601090        24.451090  96816.039234    301.216331       0.303866               1.133107             0.168138               5.682019               8.285258              3.465034              6.398072              197.662490                1.239518            1.449290                  2.603239              29163.1752               911.0\n",
      "126  2025    7                 7.441383                5.122812    297.018462        23.868462  96924.012990    300.340680       0.318382               1.136600             0.169890               5.772061               8.862958              3.489443              6.986162              234.174021                0.551577            1.452597                  3.090896              29329.6752               914.0\n",
      "127  2025    8                 7.752891                5.411113    297.648800        24.498800  96861.549368    300.089879       0.340015               1.133492             0.163478               6.200403               9.358400              3.872153              7.366600              264.106760                0.351017            1.432772                  3.157997              29370.1752               915.0\n",
      "128  2025    9                 8.360129                5.869456    298.395596        25.245596  96813.287340    299.907311       0.370136               1.130129             0.159987               6.755527              10.273071              4.347940              8.058235              330.169449                0.214988            1.424345                  3.517544              29370.1752               915.0\n",
      "129  2025   10                 8.241493                5.816945    299.177633        26.027633  96650.182971    300.047076       0.371501               1.125310             0.158674               6.472290              10.540777              4.169194              8.300244              314.963181                0.380618            1.416808                  4.068487              29685.1752               920.0\n",
      "130  2025   11                 6.226687                4.421835    299.892881        26.742881  96476.259306    300.601894       0.289613               1.120622             0.165007               3.995951               9.188065              2.488821              7.666800              135.269646                1.221272            1.408168                  5.192114              29685.1752               920.0\n",
      "131  2025   12                 5.789568                4.100886    299.703792        26.553792  96408.826862    300.952030       0.256272               1.120501             0.170678               3.519605               8.603233              2.098973              7.240544              108.722840                0.634517            1.411784                  5.083628              29685.1752               920.0\n",
      "\n",
      "   Estatísticas:\n",
      "               ano         mes  vel_vento_100m_ne_media  \\\n",
      "count   132.000000  132.000000               132.000000   \n",
      "mean   2020.000000    6.500000                 6.510792   \n",
      "std       3.174324    3.465203                 1.420326   \n",
      "min    2015.000000    1.000000                 2.846258   \n",
      "25%    2017.000000    3.750000                 5.559882   \n",
      "50%    2020.000000    6.500000                 6.589764   \n",
      "75%    2023.000000    9.250000                 7.644287   \n",
      "max    2025.000000   12.000000                 9.329388   \n",
      "\n",
      "       vel_vento_10m_ne_media  t2m_ne_media  temp_celsius_ne   sp_ne_media  \\\n",
      "count              132.000000    132.000000       132.000000    132.000000   \n",
      "mean                 4.708367    298.960602        25.810602  97303.888797   \n",
      "std                  1.058227      0.968334         0.968334    616.926339   \n",
      "min                  2.029405    296.828785        23.678785  96408.826862   \n",
      "25%                  4.023469    298.292239        25.142239  96929.180190   \n",
      "50%                  4.797575    299.139350        25.989350  97132.995050   \n",
      "75%                  5.547219    299.685088        26.535088  97569.697223   \n",
      "max                  7.027859    300.839506        27.689506  99197.621808   \n",
      "\n",
      "       sst_ne_media  zust_ne_media  densidade_ar_ne_media  ...  \\\n",
      "count    132.000000     132.000000             132.000000  ...   \n",
      "mean     301.113210       0.280762               1.133706  ...   \n",
      "std        0.861431       0.053776               0.008269  ...   \n",
      "min      299.643737       0.155409               1.118873  ...   \n",
      "25%      300.378353       0.241285               1.128114  ...   \n",
      "50%      301.198898       0.284571               1.132598  ...   \n",
      "75%      301.822708       0.321716               1.139042  ...   \n",
      "max      303.080767       0.393178               1.159506  ...   \n",
      "\n",
      "       vel_vento_100m_ne_p10  vel_vento_100m_ne_p90  vel_vento_10m_ne_p10  \\\n",
      "count             132.000000             132.000000            132.000000   \n",
      "mean                4.716353               8.208937              2.907929   \n",
      "std                 1.305947               1.606780              0.834942   \n",
      "min                 1.218279               3.840555              0.758503   \n",
      "25%                 3.992013               7.134143              2.474500   \n",
      "50%                 4.883106               8.429876              3.010470   \n",
      "75%                 5.818920               9.417792              3.555616   \n",
      "max                 7.063499              10.946489              4.439373   \n",
      "\n",
      "       vel_vento_10m_ne_p90  potencia_vento_100m_ne  precipitacao_mensal_mm  \\\n",
      "count            132.000000              132.000000              132.000000   \n",
      "mean               6.749676              178.509440                2.089419   \n",
      "std                1.358025              102.957741                1.911994   \n",
      "min                3.146756               13.021095                0.128422   \n",
      "25%                5.782026               96.818491                0.501959   \n",
      "50%                6.992226              162.332434                1.505055   \n",
      "75%                7.828391              256.675143                3.071486   \n",
      "max                9.066014              469.452179                8.154294   \n",
      "\n",
      "       ratio_vento_100_10  variabilidade_vento_100m  capacidade_total_ne_mw  \\\n",
      "count          132.000000                132.000000              132.000000   \n",
      "mean             1.385562                  3.492584            14848.384708   \n",
      "std              0.043035                  1.066833             8054.987903   \n",
      "min              1.280773                  1.242803             3409.425200   \n",
      "25%              1.358597                  2.607464             8236.725200   \n",
      "50%              1.385048                  3.541612            12284.305200   \n",
      "75%              1.414451                  4.230971            21322.025200   \n",
      "max              1.486620                  5.533792            29685.175200   \n",
      "\n",
      "       num_parques_ativos  \n",
      "count          132.000000  \n",
      "mean           505.659091  \n",
      "std            244.798401  \n",
      "min            117.000000  \n",
      "25%            298.500000  \n",
      "50%            461.000000  \n",
      "75%            712.000000  \n",
      "max            920.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "\n",
      "   Valores nulos por coluna:\n",
      "ano                         0\n",
      "mes                         0\n",
      "vel_vento_100m_ne_media     0\n",
      "vel_vento_10m_ne_media      0\n",
      "t2m_ne_media                0\n",
      "temp_celsius_ne             0\n",
      "sp_ne_media                 0\n",
      "sst_ne_media                0\n",
      "zust_ne_media               0\n",
      "densidade_ar_ne_media       0\n",
      "wind_shear_ne_media         0\n",
      "vel_vento_100m_ne_p10       0\n",
      "vel_vento_100m_ne_p90       0\n",
      "vel_vento_10m_ne_p10        0\n",
      "vel_vento_10m_ne_p90        0\n",
      "potencia_vento_100m_ne      0\n",
      "precipitacao_mensal_mm      0\n",
      "ratio_vento_100_10          0\n",
      "variabilidade_vento_100m    0\n",
      "capacidade_total_ne_mw      0\n",
      "num_parques_ativos          0\n",
      "dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ARQUIVO SALVO: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\dados_era5_ne_agregado.csv\n",
      "Total de registros: 132\n",
      "Período: 2015/01 a 2025/12\n",
      "\n",
      "✅ CORREÇÃO APLICADA:\n",
      "  - Filtro temporal: apenas parques ATIVOS em cada mês são incluídos\n",
      "  - Peso dinâmico: capacidade varia por mês conforme parques entram/saem\n",
      "  - Coluna 'num_parques_ativos' mostra quantos parques estavam ativos por mês\n",
      "\n",
      "Colunas meteorológicas agregadas:\n",
      "  - vel_vento_100m_ne_media\n",
      "  - vel_vento_10m_ne_media\n",
      "  - t2m_ne_media\n",
      "  - temp_celsius_ne\n",
      "  - sp_ne_media\n",
      "  - sst_ne_media\n",
      "  - zust_ne_media\n",
      "  - densidade_ar_ne_media\n",
      "  - wind_shear_ne_media\n",
      "  - vel_vento_100m_ne_p10\n",
      "  - vel_vento_100m_ne_p90\n",
      "  - vel_vento_10m_ne_p10\n",
      "  - vel_vento_10m_ne_p90\n",
      "  - potencia_vento_100m_ne\n",
      "  - precipitacao_mensal_mm\n",
      "  - ratio_vento_100_10\n",
      "  - variabilidade_vento_100m\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "## Tratamento Dados ERA5 - Agregação com Média Ponderada por Capacidade\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"AGREGACAO ERA5 - MEDIA PONDERADA POR CAPACIDADE INSTALADA (DINAMICA)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAR DADOS ERA5 POR PARQUE\n",
    "# ==============================================================================\n",
    "print(\"\\n1. Carregando dados ERA5 por parque...\")\n",
    "\n",
    "arquivo_era5 = os.path.join(pasta_dados, 'dados_era5_parques_ne.csv')\n",
    "\n",
    "if not os.path.exists(arquivo_era5):\n",
    "    print(f\"   ERRO: Arquivo não encontrado: {arquivo_era5}\")\n",
    "    print(f\"   Execute primeiro a célula de Processamento ERA5!\")\n",
    "    raise FileNotFoundError(arquivo_era5)\n",
    "\n",
    "df_era5_parques = pd.read_csv(arquivo_era5)\n",
    "\n",
    "# Converter colunas de data\n",
    "df_era5_parques['dat_entradaoperacao'] = pd.to_datetime(df_era5_parques['dat_entradaoperacao'], errors='coerce')\n",
    "df_era5_parques['dat_desativacao'] = pd.to_datetime(df_era5_parques['dat_desativacao'], errors='coerce')\n",
    "\n",
    "print(f\"   Total de registros: {len(df_era5_parques)}\")\n",
    "print(f\"   Parques únicos: {df_era5_parques['nome_parque'].nunique()}\")\n",
    "print(f\"   Período: {df_era5_parques['ano'].min()}/{df_era5_parques['mes'].min():02d} a {df_era5_parques['ano'].max()}/{df_era5_parques['mes'].max():02d}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FILTRAR PARQUES POR DATA DE OPERAÇÃO (PESO DINÂMICO)\n",
    "# ==============================================================================\n",
    "print(\"\\n2. Aplicando filtro temporal (parques ativos por mês)...\")\n",
    "\n",
    "# Criar coluna de data de referência (ano-mes como datetime)\n",
    "df_era5_parques['data_ref'] = pd.to_datetime(\n",
    "    df_era5_parques['ano'].astype(str) + '-' + df_era5_parques['mes'].astype(str).str.zfill(2) + '-01'\n",
    ")\n",
    "\n",
    "# Filtro: parque estava ATIVO naquele mês?\n",
    "# Ativo = data_ref >= data_entrada AND (data_desativacao is NULL OR data_ref < data_desativacao)\n",
    "\n",
    "registros_antes = len(df_era5_parques)\n",
    "\n",
    "df_era5_parques['ativo'] = (\n",
    "    # Parque já entrou em operação?\n",
    "    (df_era5_parques['data_ref'] >= df_era5_parques['dat_entradaoperacao']) &\n",
    "    # Parque ainda não foi desativado?\n",
    "    (\n",
    "        df_era5_parques['dat_desativacao'].isna() |\n",
    "        (df_era5_parques['data_ref'] < df_era5_parques['dat_desativacao'])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filtrar apenas registros ativos\n",
    "df_era5_ativos = df_era5_parques[df_era5_parques['ativo']].copy()\n",
    "\n",
    "registros_depois = len(df_era5_ativos)\n",
    "\n",
    "print(f\"   Registros ANTES do filtro: {registros_antes}\")\n",
    "print(f\"   Registros DEPOIS do filtro (ativos): {registros_depois}\")\n",
    "print(f\"   Registros removidos (inativos): {registros_antes - registros_depois}\")\n",
    "print(f\"   % removido: {100 * (registros_antes - registros_depois) / registros_antes:.1f}%\")\n",
    "\n",
    "# Verificar se ainda temos dados\n",
    "if len(df_era5_ativos) == 0:\n",
    "    print(f\"\\n   ERRO: Nenhum parque ativo encontrado no período!\")\n",
    "    print(f\"\\n   Verificando datas de entrada:\")\n",
    "    print(df_era5_parques.groupby('ano')['dat_entradaoperacao'].agg(['min', 'max']))\n",
    "    raise ValueError(\"Nenhum parque ativo no período\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CALCULAR MÉDIA PONDERADA POR MÊS (PESO = CAPACIDADE INSTALADA)\n",
    "# ==============================================================================\n",
    "print(\"\\n3. Calculando médias ponderadas por mês (apenas parques ativos)...\")\n",
    "\n",
    "# Variáveis meteorológicas para agregar\n",
    "vars_meteo = [\n",
    "    'vel_vento_10m',\n",
    "    'vel_vento_100m',\n",
    "    't2m',\n",
    "    'sp',\n",
    "    'tp',\n",
    "    'sst',\n",
    "    'zust',\n",
    "    'densidade_ar',\n",
    "    'wind_shear'\n",
    "]\n",
    "\n",
    "# Função para média ponderada\n",
    "def weighted_average(df, var_col, weight_col='peso_capacidade_mw'):\n",
    "    \"\"\"Calcula média ponderada de uma variável\"\"\"\n",
    "    # Filtrar valores válidos (não-NaN)\n",
    "    valid = df[[var_col, weight_col]].dropna()\n",
    "    if len(valid) == 0:\n",
    "        return np.nan\n",
    "    return (valid[var_col] * valid[weight_col]).sum() / valid[weight_col].sum()\n",
    "\n",
    "# Agrupar por ano e mês\n",
    "df_agregado = df_era5_ativos.groupby(['ano', 'mes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        **{f'{var}_ne_media': weighted_average(x, var) for var in vars_meteo},\n",
    "        'capacidade_total_ne_mw': x['peso_capacidade_mw'].sum(),\n",
    "        'num_parques_ativos': x['nome_parque'].nunique()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(f\"\\n   Total de meses processados: {len(df_agregado)}\")\n",
    "print(f\"\\n   Primeiras 10 linhas:\")\n",
    "print(df_agregado.head(10).to_string())\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CALCULAR PERCENTIS DE VENTO (P10, P90) PARA CAPTURAR VARIABILIDADE\n",
    "# ==============================================================================\n",
    "print(\"\\n4. Calculando percentis de vento...\")\n",
    "\n",
    "# Para cada mês, calcular P10 e P90 da velocidade do vento ponderados\n",
    "def weighted_percentile(df, var_col, percentile, weight_col='peso_capacidade_mw'):\n",
    "    \"\"\"Calcula percentil ponderado\"\"\"\n",
    "    valid = df[[var_col, weight_col]].dropna()\n",
    "    if len(valid) == 0:\n",
    "        return np.nan\n",
    "    sorted_df = valid.sort_values(var_col)\n",
    "    cumsum = sorted_df[weight_col].cumsum()\n",
    "    cutoff = sorted_df[weight_col].sum() * (percentile / 100.0)\n",
    "    idx = (cumsum >= cutoff).idxmax()\n",
    "    return sorted_df.loc[idx, var_col]\n",
    "\n",
    "df_percentis = df_era5_ativos.groupby(['ano', 'mes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'vel_vento_100m_ne_p10': weighted_percentile(x, 'vel_vento_100m', 10),\n",
    "        'vel_vento_100m_ne_p90': weighted_percentile(x, 'vel_vento_100m', 90),\n",
    "        'vel_vento_10m_ne_p10': weighted_percentile(x, 'vel_vento_10m', 10),\n",
    "        'vel_vento_10m_ne_p90': weighted_percentile(x, 'vel_vento_10m', 90),\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Merge com df_agregado\n",
    "df_agregado = df_agregado.merge(df_percentis, on=['ano', 'mes'], how='left')\n",
    "\n",
    "print(f\"   Percentis calculados para {len(df_agregado)} meses\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ADICIONAR FEATURES DERIVADAS\n",
    "# ==============================================================================\n",
    "print(\"\\n5. Calculando features derivadas...\")\n",
    "\n",
    "# Potência do vento (0.5 * densidade * v^3)\n",
    "df_agregado['potencia_vento_100m_ne'] = (\n",
    "    0.5 * df_agregado['densidade_ar_ne_media'] * (df_agregado['vel_vento_100m_ne_media'] ** 3)\n",
    ")\n",
    "\n",
    "# Temperatura em Celsius (para facilitar interpretação)\n",
    "df_agregado['temp_celsius_ne'] = df_agregado['t2m_ne_media'] - 273.15\n",
    "\n",
    "# Precipitação acumulada mensal (tp já é média mensal em m)\n",
    "df_agregado['precipitacao_mensal_mm'] = df_agregado['tp_ne_media'] * 1000  # converter m para mm\n",
    "\n",
    "# Ratio de vento (100m vs 10m)\n",
    "df_agregado['ratio_vento_100_10'] = (\n",
    "    df_agregado['vel_vento_100m_ne_media'] / df_agregado['vel_vento_10m_ne_media'].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "# Variabilidade do vento (P90 - P10)\n",
    "df_agregado['variabilidade_vento_100m'] = (\n",
    "    df_agregado['vel_vento_100m_ne_p90'] - df_agregado['vel_vento_100m_ne_p10']\n",
    ")\n",
    "\n",
    "print(f\"   Features derivadas calculadas\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. ORDENAR E SELECIONAR COLUNAS FINAIS\n",
    "# ==============================================================================\n",
    "print(\"\\n6. Preparando dataset final...\")\n",
    "\n",
    "colunas_finais = [\n",
    "    'ano', 'mes',\n",
    "    # Médias ponderadas\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'vel_vento_10m_ne_media',\n",
    "    't2m_ne_media',\n",
    "    'temp_celsius_ne',\n",
    "    'sp_ne_media',\n",
    "    'sst_ne_media',\n",
    "    'zust_ne_media',\n",
    "    'densidade_ar_ne_media',\n",
    "    'wind_shear_ne_media',\n",
    "    # Percentis\n",
    "    'vel_vento_100m_ne_p10',\n",
    "    'vel_vento_100m_ne_p90',\n",
    "    'vel_vento_10m_ne_p10',\n",
    "    'vel_vento_10m_ne_p90',\n",
    "    # Derivadas\n",
    "    'potencia_vento_100m_ne',\n",
    "    'precipitacao_mensal_mm',\n",
    "    'ratio_vento_100_10',\n",
    "    'variabilidade_vento_100m',\n",
    "    # Metadados\n",
    "    'capacidade_total_ne_mw',\n",
    "    'num_parques_ativos'\n",
    "]\n",
    "\n",
    "df_agregado = df_agregado[colunas_finais]\n",
    "df_agregado = df_agregado.sort_values(['ano', 'mes']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n   Dataset final: {df_agregado.shape}\")\n",
    "print(f\"\\n   Primeiras 10 linhas:\")\n",
    "print(df_agregado.head(10).to_string())\n",
    "print(f\"\\n   Últimas 10 linhas:\")\n",
    "print(df_agregado.tail(10).to_string())\n",
    "\n",
    "print(f\"\\n   Estatísticas:\")\n",
    "print(df_agregado.describe())\n",
    "\n",
    "print(f\"\\n   Valores nulos por coluna:\")\n",
    "print(df_agregado.isnull().sum())\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. SALVAR\n",
    "# ==============================================================================\n",
    "output_file = os.path.join(pasta_dados, 'dados_era5_ne_agregado.csv')\n",
    "df_agregado.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(f\"ARQUIVO SALVO: {output_file}\")\n",
    "print(f\"Total de registros: {len(df_agregado)}\")\n",
    "print(f\"Período: {df_agregado['ano'].min()}/{df_agregado['mes'].min():02d} a {df_agregado['ano'].max()}/{df_agregado['mes'].max():02d}\")\n",
    "print(f\"\\n✅ CORREÇÃO APLICADA:\")\n",
    "print(f\"  - Filtro temporal: apenas parques ATIVOS em cada mês são incluídos\")\n",
    "print(f\"  - Peso dinâmico: capacidade varia por mês conforme parques entram/saem\")\n",
    "print(f\"  - Coluna 'num_parques_ativos' mostra quantos parques estavam ativos por mês\")\n",
    "print(f\"\\nColunas meteorológicas agregadas:\")\n",
    "for col in colunas_finais:\n",
    "    if col not in ['ano', 'mes', 'capacidade_total_ne_mw', 'num_parques_ativos']:\n",
    "        print(f\"  - {col}\")\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d95341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DATASET FINAL - ONS + DADOS METEOROLOGICOS POR PARQUE\n",
      "====================================================================================================\n",
      "1. Carregando datasets...\n",
      "   Dados ONS: (132, 14)\n",
      "   Período: 2015/01 a 2025/12\n",
      "   Dados ERA5 por parque: (319704, 26)\n",
      "   Parques únicos: 2403\n",
      "2. Preparando pivot dos dados ERA5...\n",
      "   Células únicas (localizações): 225\n",
      "   Dados agregados por célula: (29700, 13)\n",
      "3. Pivotando células para colunas...\n",
      "   ERA5 pivotado: (132, 2252)\n",
      "   Total de colunas meteorológicas: 2250\n",
      "4. Fazendo merge ONS + ERA5...\n",
      "   Dataset após merge: (132, 2264)\n",
      "   Linhas (meses): 132\n",
      "   Colunas totais: 2264\n",
      "5. Criando coluna de data...\n",
      "6. Estatísticas do dataset final...\n",
      "Shape final: (132, 2265)\n",
      "   Período: 2015-01-01 00:00:00 a 2025-12-01 00:00:00\n",
      "Colunas por categoria:\n",
      "     - Data: 1\n",
      "     - ONS (operação): 14\n",
      "     - Meteorológicas: 2025\n",
      "     - Capacidade por célula: 225\n",
      "Target para modelagem: 'geracao_eolica_ne_mwmed'\n",
      "Colunas com valores nulos: 552\n",
      "   (SST tem nulos em células continentais - normal)\n",
      "7. Salvando dataset final...\n",
      "   CSV: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\dataset_final_modelagem.csv\n",
      "   Parquet: C:\\\\Users\\\\Admin\\\\Documents\\\\Puc\\\\IC\\\\Dados\\dataset_final_modelagem.parquet\n",
      "====================================================================================================\n",
      "DATASET FINAL CRIADO!\n",
      "====================================================================================================\n",
      "Formato do dataset:\n",
      "  - Linhas: 132 meses (2015-01 a 2025-12)\n",
      "  - Colunas: 2265 (ONS + meteorológicas por célula)\n",
      "Target: geracao_eolica_ne_mwmed\n",
      "Features: dados meteorológicos de 225 células/localizações\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Junção de datasets - Dados ONS + ERA5 por Parque (expandido em colunas)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DATASET FINAL - ONS + DADOS METEOROLOGICOS POR PARQUE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAR DATASETS\n",
    "# ==============================================================================\n",
    "print(\"1. Carregando datasets...\")\n",
    "\n",
    "# Dados ONS (geração, capacidade, demanda) - será nosso target\n",
    "arquivo_ons = os.path.join(pasta_dados, 'dados_operacao_ONS.csv')\n",
    "df_ons = pd.read_csv(arquivo_ons)\n",
    "print(f\"   Dados ONS: {df_ons.shape}\")\n",
    "print(f\"   Período: {df_ons['ano'].min()}/{df_ons['mes'].min():02d} a {df_ons['ano'].max()}/{df_ons['mes'].max():02d}\")\n",
    "\n",
    "# Dados ERA5 por parque (NÃO agregados)\n",
    "arquivo_era5 = os.path.join(pasta_dados, 'dados_era5_parques_ne.csv')\n",
    "df_era5 = pd.read_csv(arquivo_era5)\n",
    "print(f\"   Dados ERA5 por parque: {df_era5.shape}\")\n",
    "print(f\"   Parques únicos: {df_era5['nome_parque'].nunique()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PREPARAR DADOS ERA5 PARA PIVOT (PARQUES -> COLUNAS)\n",
    "# ==============================================================================\n",
    "print(\"2. Preparando pivot dos dados ERA5...\")\n",
    "\n",
    "# Variáveis meteorológicas que queremos como features\n",
    "vars_meteo = ['vel_vento_100m', 'vel_vento_10m', 't2m', 'sp', 'tp', 'sst', 'zust', \n",
    "              'densidade_ar', 'wind_shear']\n",
    "\n",
    "# Criar identificador único para cada parque (usar cell_key que agrupa parques na mesma célula)\n",
    "# Para não ter milhares de colunas, vamos usar cell_key (225 células únicas)\n",
    "df_era5['cell_id'] = df_era5['cell_key']\n",
    "\n",
    "print(f\"   Células únicas (localizações): {df_era5['cell_id'].nunique()}\")\n",
    "\n",
    "# Para cada célula, pegar a média ponderada por capacidade dos parques naquela célula\n",
    "# Isso mantém a granularidade espacial sem ter uma coluna por parque\n",
    "df_era5_cell = df_era5.groupby(['ano', 'mes', 'cell_id']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        **{var: np.average(x[var].dropna(), weights=x.loc[x[var].notna(), 'peso_capacidade_mw']) \n",
    "           if len(x[var].dropna()) > 0 and x.loc[x[var].notna(), 'peso_capacidade_mw'].sum() > 0 \n",
    "           else np.nan \n",
    "           for var in vars_meteo},\n",
    "        'capacidade_mw': x['peso_capacidade_mw'].sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(f\"   Dados agregados por célula: {df_era5_cell.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PIVOT - CADA CÉLULA VIRA UM CONJUNTO DE COLUNAS\n",
    "# ==============================================================================\n",
    "print(\"3. Pivotando células para colunas...\")\n",
    "\n",
    "# Criar dataframe pivotado\n",
    "dfs_pivot = []\n",
    "\n",
    "for var in vars_meteo + ['capacidade_mw']:\n",
    "    df_pivot = df_era5_cell.pivot(\n",
    "        index=['ano', 'mes'], \n",
    "        columns='cell_id', \n",
    "        values=var\n",
    "    )\n",
    "    # Renomear colunas: var_cellkey\n",
    "    df_pivot.columns = [f\"{var}_{col}\" for col in df_pivot.columns]\n",
    "    dfs_pivot.append(df_pivot)\n",
    "\n",
    "# Concatenar todos os pivots\n",
    "df_era5_wide = pd.concat(dfs_pivot, axis=1).reset_index()\n",
    "\n",
    "print(f\"   ERA5 pivotado: {df_era5_wide.shape}\")\n",
    "print(f\"   Total de colunas meteorológicas: {len(df_era5_wide.columns) - 2}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MERGE COM DADOS ONS\n",
    "# ==============================================================================\n",
    "print(\"4. Fazendo merge ONS + ERA5...\")\n",
    "\n",
    "df_final = df_ons.merge(df_era5_wide, on=['ano', 'mes'], how='inner')\n",
    "\n",
    "print(f\"   Dataset após merge: {df_final.shape}\")\n",
    "print(f\"   Linhas (meses): {len(df_final)}\")\n",
    "print(f\"   Colunas totais: {len(df_final.columns)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. CRIAR COLUNA DE DATA\n",
    "# ==============================================================================\n",
    "print(\"5. Criando coluna de data...\")\n",
    "\n",
    "df_final['data'] = pd.to_datetime(\n",
    "    df_final['ano'].astype(str) + '-' + df_final['mes'].astype(str).str.zfill(2) + '-01'\n",
    ")\n",
    "\n",
    "# Reordenar colunas\n",
    "cols_ons = [c for c in df_ons.columns if c in df_final.columns]\n",
    "cols_meteo = [c for c in df_final.columns if c not in cols_ons and c != 'data']\n",
    "df_final = df_final[['data'] + cols_ons + cols_meteo]\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. ESTATÍSTICAS\n",
    "# ==============================================================================\n",
    "print(\"6. Estatísticas do dataset final...\")\n",
    "\n",
    "print(f\"Shape final: {df_final.shape}\")\n",
    "print(f\"   Período: {df_final['data'].min()} a {df_final['data'].max()}\")\n",
    "\n",
    "# Contar colunas por categoria\n",
    "n_ons = len(cols_ons)\n",
    "n_meteo = len([c for c in df_final.columns if any(v in c for v in vars_meteo)])\n",
    "n_cap = len([c for c in df_final.columns if 'capacidade_mw_' in c])\n",
    "\n",
    "print(f\"Colunas por categoria:\")\n",
    "print(f\"     - Data: 1\")\n",
    "print(f\"     - ONS (operação): {n_ons}\")\n",
    "print(f\"     - Meteorológicas: {n_meteo}\")\n",
    "print(f\"     - Capacidade por célula: {n_cap}\")\n",
    "\n",
    "print(f\"Target para modelagem: 'geracao_eolica_ne_mwmed'\")\n",
    "\n",
    "# Valores nulos\n",
    "nulos = df_final.isnull().sum()\n",
    "cols_com_nulos = nulos[nulos > 0]\n",
    "print(f\"Colunas com valores nulos: {len(cols_com_nulos)}\")\n",
    "if len(cols_com_nulos) > 0:\n",
    "    print(f\"   (SST tem nulos em células continentais - normal)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. SALVAR DATASET FINAL\n",
    "# ==============================================================================\n",
    "print(\"7. Salvando dataset final...\")\n",
    "\n",
    "output_csv = os.path.join(pasta_dados, 'dataset_final_modelagem.csv')\n",
    "df_final.to_csv(output_csv, index=False)\n",
    "print(f\"   CSV: {output_csv}\")\n",
    "\n",
    "output_parquet = os.path.join(pasta_dados, 'dataset_final_modelagem.parquet')\n",
    "df_final.to_parquet(output_parquet, index=False)\n",
    "print(f\"   Parquet: {output_parquet}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RESUMO\n",
    "# ==============================================================================\n",
    "print(\"\" + \"=\"*100)\n",
    "print(\"DATASET FINAL CRIADO!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Formato do dataset:\")\n",
    "print(f\"  - Linhas: {len(df_final)} meses (2015-01 a 2025-12)\")\n",
    "print(f\"  - Colunas: {len(df_final.columns)} (ONS + meteorológicas por célula)\")\n",
    "print(f\"Target: geracao_eolica_ne_mwmed\")\n",
    "print(f\"Features: dados meteorológicos de {df_era5['cell_id'].nunique()} células/localizações\")\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1b459ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparação do INPUT do modelo (algoritmos de padronização minMax...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "195152a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "EDA COMPLETA - DATASET FINAL DE MODELAGEM\n",
      "====================================================================================================\n",
      "\n",
      "1. CARREGAMENTO E VALIDAÇÃO DO DICIONÁRIO DE DADOS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ Dataset carregado: 132 linhas × 2265 colunas\n",
      "✓ Dicionário de dados salvo em: validacao/00_dicionario_dados.txt\n",
      "\n",
      "2. CONSISTÊNCIA E INTEGRIDADE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ Sequência temporal: 0 meses faltantes\n",
      "✓ Duplicatas: 0 registros duplicados\n",
      "✓ Valores faltantes: 72864 células (24.37%)\n",
      "✓ Relatório de outliers salvo em: validacao/01_outliers_report.csv\n",
      "✓ Relatório de integridade salvo em: validacao/02_integridade.txt\n",
      "\n",
      "3. SÉRIES TEMPORAIS PRINCIPAIS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ 8 séries temporais salvas em: series_temporais/\n",
      "\n",
      "4. ESTATÍSTICAS DESCRITIVAS COMPLETAS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ Estatísticas descritivas salvas em: estatisticas/01_estatisticas_descritivas.csv\n",
      "✓ Gráfico de distribuições salvo em: estatisticas/02_distribuicoes.png\n",
      "\n",
      "5. TENDÊNCIAS E TAXAS DE CRESCIMENTO\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ Análise de tendências salva em: estatisticas/03_tendencias.csv\n",
      "✓ Gráfico de crescimento YoY salvo em: estatisticas/04_crescimento_yoy.png\n",
      "\n",
      "6. SAZONALIDADE POR MÊS DO ANO\n",
      "----------------------------------------------------------------------------------------------------\n",
      "✓ Boxplots mensais salvos em: sazonalidade/01_boxplots_mensais.png\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'temp_celsius_ne', 'vel_vento_100m_ne_media'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 485\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Boxplots mensais salvos em: sazonalidade/01_boxplots_mensais.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# Perfis sazonais (médias mensais)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m perfis_sazonais = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariaveis_sazonalidade\u001b[49m\u001b[43m]\u001b[49m.mean()\n\u001b[32m    486\u001b[39m perfis_sazonais.to_csv(os.path.join(EDA_DIR, \u001b[33m'\u001b[39m\u001b[33msazonalidade\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m02_perfis_sazonais.csv\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# Gráfico de perfis sazonais\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Puc\\IC\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[39m, in \u001b[36mDataFrameGroupBy.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) > \u001b[32m1\u001b[39m:\n\u001b[32m   1945\u001b[39m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[32m   1946\u001b[39m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[32m   1947\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1949\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1950\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Puc\\IC\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:240\u001b[39m, in \u001b[36mSelectionMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj.columns.intersection(key)) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)):\n\u001b[32m    239\u001b[39m         bad_keys = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(key).difference(\u001b[38;5;28mself\u001b[39m.obj.columns))\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(bad_keys)[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gotitem(\u001b[38;5;28mlist\u001b[39m(key), ndim=\u001b[32m2\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: \"Columns not found: 'temp_celsius_ne', 'vel_vento_100m_ne_media'\""
     ]
    }
   ],
   "source": [
    "#EDA COMPLETA - DATASET FINAL DE MODELAGEM\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURAÇÕES\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos\n",
    "DADOS_DIR = r'C:\\Users\\Admin\\Documents\\Puc\\IC\\Dados'\n",
    "DATASET_FILE = os.path.join(DADOS_DIR, 'dataset_final_modelagem.parquet')\n",
    "EDA_DIR = os.path.join(DADOS_DIR, 'EDA_Completa')\n",
    "os.makedirs(EDA_DIR, exist_ok=True)\n",
    "\n",
    "# Criar subdiretórios\n",
    "subdirs = [\n",
    "    'validacao',\n",
    "    'series_temporais',\n",
    "    'estatisticas',\n",
    "    'sazonalidade',\n",
    "    'mapas_espaciais',\n",
    "    'heatmaps',\n",
    "    'decomposicao',\n",
    "    'correlacoes',\n",
    "    'extremos',\n",
    "    'indicadores',\n",
    "    'hipoteses'\n",
    "]\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(EDA_DIR, subdir), exist_ok=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"EDA COMPLETA - DATASET FINAL DE MODELAGEM\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARREGAMENTO E VALIDAÇÃO DO DICIONÁRIO DE DADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n1. CARREGAMENTO E VALIDAÇÃO DO DICIONÁRIO DE DADOS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Carregar dataset\n",
    "df = pd.read_parquet(DATASET_FILE)\n",
    "print(f\"✓ Dataset carregado: {df.shape[0]} linhas × {df.shape[1]} colunas\")\n",
    "\n",
    "# Converter data para datetime se necessário\n",
    "if 'data' not in df.columns and 'ano' in df.columns and 'mes' in df.columns:\n",
    "    df['data'] = pd.to_datetime(df['ano'].astype(str) + '-' + df['mes'].astype(str).str.zfill(2) + '-01')\n",
    "\n",
    "df = df.sort_values('data').reset_index(drop=True)\n",
    "\n",
    "# Dicionário de dados\n",
    "dicionario_dados = {\n",
    "    # Temporais\n",
    "    'data': {'unidade': 'datetime', 'descricao': 'Data de referência (YYYY-MM-01)', 'tipo': 'temporal'},\n",
    "    'ano': {'unidade': 'ano', 'descricao': 'Ano', 'tipo': 'temporal'},\n",
    "    'mes': {'unidade': 'mês', 'descricao': 'Mês (1-12)', 'tipo': 'temporal'},\n",
    "\n",
    "    # ONS - Geração e Capacidade\n",
    "    'geracao_eolica_ne_mwmed': {'unidade': 'MWmed', 'descricao': 'Geração eólica média mensal NE', 'tipo': 'target'},\n",
    "    'capacidade_eolica_ne_mw': {'unidade': 'MW', 'descricao': 'Capacidade instalada acumulada NE', 'tipo': 'ons'},\n",
    "    'crescimento_capacidade_mw': {'unidade': 'MW', 'descricao': 'Capacidade adicionada no mês', 'tipo': 'ons'},\n",
    "\n",
    "    # ONS - Demanda\n",
    "    'carga_ne_mwmed': {'unidade': 'MWmed', 'descricao': 'Carga média mensal do NE', 'tipo': 'ons'},\n",
    "    'demanda_sin_mwmed': {'unidade': 'MWmed', 'descricao': 'Demanda média mensal do SIN', 'tipo': 'ons'},\n",
    "    'demanda_max_sin_mw': {'unidade': 'MW', 'descricao': 'Pico de demanda mensal do SIN', 'tipo': 'ons'},\n",
    "\n",
    "    # ONS - Indicadores\n",
    "    'corte_eolica_ne_mwmed': {'unidade': 'MWmed', 'descricao': 'Curtailment estimado (CF=45%)', 'tipo': 'ons'},\n",
    "    'capacity_factor_ne': {'unidade': '%', 'descricao': 'Fator de capacidade mensal NE', 'tipo': 'indicador'},\n",
    "    'penetracao_eolica_ne': {'unidade': '%', 'descricao': 'Penetração eólica no NE', 'tipo': 'indicador'},\n",
    "    'penetracao_eolica_sin': {'unidade': '%', 'descricao': 'Penetração eólica no SIN', 'tipo': 'indicador'},\n",
    "\n",
    "    # ERA5 - Vento\n",
    "    'vel_vento_100m_ne_media': {'unidade': 'm/s', 'descricao': 'Velocidade média do vento a 100m', 'tipo': 'era5'},\n",
    "    'vel_vento_10m_ne_media': {'unidade': 'm/s', 'descricao': 'Velocidade média do vento a 10m', 'tipo': 'era5'},\n",
    "    'vel_vento_100m_ne_p10': {'unidade': 'm/s', 'descricao': 'Percentil 10 do vento a 100m', 'tipo': 'era5'},\n",
    "    'vel_vento_100m_ne_p90': {'unidade': 'm/s', 'descricao': 'Percentil 90 do vento a 100m', 'tipo': 'era5'},\n",
    "    'variabilidade_vento_100m': {'unidade': 'm/s', 'descricao': 'Variabilidade do vento (P90-P10)', 'tipo': 'era5'},\n",
    "\n",
    "    # ERA5 - Atmosféricos\n",
    "    't2m_ne_media': {'unidade': 'K', 'descricao': 'Temperatura média a 2m', 'tipo': 'era5'},\n",
    "    'temp_celsius_ne': {'unidade': '°C', 'descricao': 'Temperatura média em Celsius', 'tipo': 'era5'},\n",
    "    'sp_ne_media': {'unidade': 'Pa', 'descricao': 'Pressão superficial média', 'tipo': 'era5'},\n",
    "    'densidade_ar_ne_media': {'unidade': 'kg/m³', 'descricao': 'Densidade do ar média', 'tipo': 'era5'},\n",
    "    'precipitacao_mensal_mm': {'unidade': 'mm', 'descricao': 'Precipitação acumulada mensal', 'tipo': 'era5'},\n",
    "\n",
    "    # ERA5 - Derivadas\n",
    "    'potencia_vento_100m_ne': {'unidade': 'W/m²', 'descricao': 'Potência do vento (0.5*ρ*v³)', 'tipo': 'derivada'},\n",
    "    'wind_shear_ne_media': {'unidade': 'adimensional', 'descricao': 'Wind shear médio', 'tipo': 'era5'},\n",
    "    'ratio_vento_100_10': {'unidade': 'adimensional', 'descricao': 'Razão vento 100m/10m', 'tipo': 'derivada'},\n",
    "}\n",
    "\n",
    "# Relatório de validação\n",
    "with open(os.path.join(EDA_DIR, 'validacao', '00_dicionario_dados.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"DICIONÁRIO DE DADOS - DATASET FINAL DE MODELAGEM\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Dataset: {DATASET_FILE}\\n\")\n",
    "    f.write(f\"Dimensões: {df.shape[0]} linhas × {df.shape[1]} colunas\\n\")\n",
    "    f.write(f\"Período: {df['data'].min().strftime('%Y-%m')} a {df['data'].max().strftime('%Y-%m')}\\n\")\n",
    "    f.write(f\"Meses totais: {len(df)}\\n\\n\")\n",
    "\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    f.write(\"VARIÁVEIS DO DATASET\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\\n\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in dicionario_dados:\n",
    "            info = dicionario_dados[col]\n",
    "            f.write(f\"• {col}\\n\")\n",
    "            f.write(f\"  Tipo: {info['tipo']}\\n\")\n",
    "            f.write(f\"  Unidade: {info['unidade']}\\n\")\n",
    "            f.write(f\"  Descrição: {info['descricao']}\\n\")\n",
    "            f.write(f\"  Dtype: {df[col].dtype}\\n\")\n",
    "            f.write(f\"  Nulos: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.2f}%)\\n\\n\")\n",
    "        else:\n",
    "            f.write(f\"• {col}\\n\")\n",
    "            f.write(f\"  [Sem descrição no dicionário]\\n\")\n",
    "            f.write(f\"  Dtype: {df[col].dtype}\\n\")\n",
    "            f.write(f\"  Nulos: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.2f}%)\\n\\n\")\n",
    "\n",
    "print(f\"✓ Dicionário de dados salvo em: validacao/00_dicionario_dados.txt\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CONSISTÊNCIA E INTEGRIDADE\n",
    "# ==============================================================================\n",
    "print(\"\\n2. CONSISTÊNCIA E INTEGRIDADE\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Verificar sequência temporal\n",
    "datas_esperadas = pd.date_range(start=df['data'].min(), end=df['data'].max(), freq='MS')\n",
    "datas_faltantes = set(datas_esperadas) - set(df['data'])\n",
    "\n",
    "print(f\"✓ Sequência temporal: {len(datas_faltantes)} meses faltantes\")\n",
    "\n",
    "# Verificar duplicatas\n",
    "duplicatas = df.duplicated(subset=['ano', 'mes']).sum()\n",
    "print(f\"✓ Duplicatas: {duplicatas} registros duplicados\")\n",
    "\n",
    "# Estatísticas de missing\n",
    "missing_por_coluna = df.isnull().sum()\n",
    "missing_total = missing_por_coluna.sum()\n",
    "print(f\"✓ Valores faltantes: {missing_total} células ({missing_total/(len(df)*len(df.columns))*100:.2f}%)\")\n",
    "\n",
    "# Detectar outliers (método IQR)\n",
    "def detectar_outliers_iqr(serie):\n",
    "    Q1 = serie.quantile(0.25)\n",
    "    Q3 = serie.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return ((serie < lower) | (serie > upper)).sum()\n",
    "\n",
    "# Detectar outliers (método Z-score)\n",
    "def detectar_outliers_zscore(serie, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(serie.dropna()))\n",
    "    return (z_scores > threshold).sum()\n",
    "\n",
    "# Relatório de outliers\n",
    "colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "colunas_para_outliers = [col for col in colunas_numericas if col not in ['ano', 'mes']]\n",
    "\n",
    "outliers_report = []\n",
    "for col in colunas_para_outliers[:20]:  # Limitar para não sobrecarregar\n",
    "    outliers_iqr = detectar_outliers_iqr(df[col])\n",
    "    outliers_z = detectar_outliers_zscore(df[col])\n",
    "    outliers_report.append({\n",
    "        'Variável': col,\n",
    "        'Outliers (IQR)': outliers_iqr,\n",
    "        'Outliers (Z-score>3)': outliers_z\n",
    "    })\n",
    "\n",
    "df_outliers = pd.DataFrame(outliers_report)\n",
    "df_outliers.to_csv(os.path.join(EDA_DIR, 'validacao', '01_outliers_report.csv'), index=False)\n",
    "print(f\"✓ Relatório de outliers salvo em: validacao/01_outliers_report.csv\")\n",
    "\n",
    "# Salvar relatório de integridade\n",
    "with open(os.path.join(EDA_DIR, 'validacao', '02_integridade.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"RELATÓRIO DE INTEGRIDADE E CONSISTÊNCIA\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"1. COBERTURA TEMPORAL\\n\")\n",
    "    f.write(f\"   Período: {df['data'].min().strftime('%Y-%m')} a {df['data'].max().strftime('%Y-%m')}\\n\")\n",
    "    f.write(f\"   Meses esperados: {len(datas_esperadas)}\\n\")\n",
    "    f.write(f\"   Meses presentes: {len(df)}\\n\")\n",
    "    f.write(f\"   Meses faltantes: {len(datas_faltantes)}\\n\")\n",
    "    if datas_faltantes:\n",
    "        f.write(f\"   Datas faltantes: {sorted([d.strftime('%Y-%m') for d in datas_faltantes])}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"2. DUPLICATAS\\n\")\n",
    "    f.write(f\"   Registros duplicados: {duplicatas}\\n\\n\")\n",
    "\n",
    "    f.write(\"3. VALORES FALTANTES\\n\")\n",
    "    for col, count in missing_por_coluna[missing_por_coluna > 0].items():\n",
    "        f.write(f\"   {col}: {count} ({count/len(df)*100:.2f}%)\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"4. OUTLIERS (TOP 10 variáveis com mais outliers)\\n\")\n",
    "    top_outliers = df_outliers.sort_values('Outliers (IQR)', ascending=False).head(10)\n",
    "    f.write(top_outliers.to_string(index=False))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"✓ Relatório de integridade salvo em: validacao/02_integridade.txt\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SÉRIES TEMPORAIS PRINCIPAIS (Estilo ONS)\n",
    "# ==============================================================================\n",
    "print(\"\\n3. SÉRIES TEMPORAIS PRINCIPAIS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Variáveis principais para plotar\n",
    "variaveis_principais = {\n",
    "    'capacidade_eolica_ne_mw': 'Capacidade Eólica Instalada NE (MW)',\n",
    "    'geracao_eolica_ne_mwmed': 'Geração Eólica Média Mensal NE (MWmed)',\n",
    "    'carga_ne_mwmed': 'Carga Média Mensal NE (MWmed)',\n",
    "    'demanda_sin_mwmed': 'Demanda Média Mensal SIN (MWmed)',\n",
    "    'demanda_max_sin_mw': 'Demanda Máxima Mensal SIN (MW)',\n",
    "    'corte_eolica_ne_mwmed': 'Curtailment Estimado NE (MWmed)',\n",
    "    'capacity_factor_ne': 'Fator de Capacidade NE (%)',\n",
    "    'vel_vento_100m_ne_media': 'Velocidade do Vento a 100m NE (m/s)'\n",
    "}\n",
    "\n",
    "for i, (col, titulo) in enumerate(variaveis_principais.items(), 1):\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "    ax.plot(df['data'], df[col], linewidth=2, marker='o', markersize=4, label=titulo)\n",
    "\n",
    "    # Adicionar linha de tendência\n",
    "    z = np.polyfit(range(len(df)), df[col].fillna(df[col].mean()), 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df['data'], p(range(len(df))),\n",
    "            linestyle='--', linewidth=2, color='red', alpha=0.7, label='Tendência Linear')\n",
    "\n",
    "    ax.set_xlabel('Data', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(titulo.split('(')[1].replace(')', ''), fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{titulo}\\nPeríodo: {df[\"data\"].min().strftime(\"%b/%Y\")} a {df[\"data\"].max().strftime(\"%b/%Y\")}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EDA_DIR, 'series_temporais', f'{i:02d}_{col}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✓ {len(variaveis_principais)} séries temporais salvas em: series_temporais/\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ESTATÍSTICAS DESCRITIVAS COMPLETAS\n",
    "# ==============================================================================\n",
    "print(\"\\n4. ESTATÍSTICAS DESCRITIVAS COMPLETAS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "def estatisticas_completas(serie):\n",
    "    \"\"\"Calcula estatísticas descritivas completas para uma série\"\"\"\n",
    "    serie_clean = serie.dropna()\n",
    "\n",
    "    return {\n",
    "        'count': len(serie_clean),\n",
    "        'mean': serie_clean.mean(),\n",
    "        'median': serie_clean.median(),\n",
    "        'min': serie_clean.min(),\n",
    "        'max': serie_clean.max(),\n",
    "        'std': serie_clean.std(),\n",
    "        'var': serie_clean.var(),\n",
    "        'q25': serie_clean.quantile(0.25),\n",
    "        'q75': serie_clean.quantile(0.75),\n",
    "        'iqr': serie_clean.quantile(0.75) - serie_clean.quantile(0.25),\n",
    "        'p10': serie_clean.quantile(0.10),\n",
    "        'p50': serie_clean.quantile(0.50),\n",
    "        'p90': serie_clean.quantile(0.90),\n",
    "        'cv': serie_clean.std() / serie_clean.mean() if serie_clean.mean() != 0 else np.nan,\n",
    "        'skewness': stats.skew(serie_clean),\n",
    "        'kurtosis': stats.kurtosis(serie_clean),\n",
    "        'range': serie_clean.max() - serie_clean.min(),\n",
    "    }\n",
    "\n",
    "# Calcular estatísticas para variáveis principais\n",
    "stats_results = []\n",
    "for col in colunas_para_outliers[:30]:  # Top 30 variáveis\n",
    "    if col in df.columns:\n",
    "        stats_dict = estatisticas_completas(df[col])\n",
    "        stats_dict['variavel'] = col\n",
    "        stats_results.append(stats_dict)\n",
    "\n",
    "df_stats = pd.DataFrame(stats_results)\n",
    "df_stats = df_stats[['variavel', 'count', 'mean', 'median', 'std', 'min', 'max',\n",
    "                      'q25', 'q75', 'iqr', 'p10', 'p50', 'p90',\n",
    "                      'cv', 'skewness', 'kurtosis', 'range']]\n",
    "\n",
    "df_stats.to_csv(os.path.join(EDA_DIR, 'estatisticas', '01_estatisticas_descritivas.csv'), index=False)\n",
    "print(f\"✓ Estatísticas descritivas salvas em: estatisticas/01_estatisticas_descritivas.csv\")\n",
    "\n",
    "# Criar visualização das distribuições\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "variaveis_distribuicao = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'capacity_factor_ne',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'carga_ne_mwmed',\n",
    "    'penetracao_eolica_ne',\n",
    "    'corte_eolica_ne_mwmed',\n",
    "    'temp_celsius_ne',\n",
    "    'densidade_ar_ne_media'\n",
    "]\n",
    "\n",
    "for i, col in enumerate(variaveis_distribuicao):\n",
    "    if col in df.columns and i < len(axes):\n",
    "        ax = axes[i]\n",
    "        data = df[col].dropna()\n",
    "\n",
    "        # Histograma + KDE\n",
    "        ax.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # KDE\n",
    "        try:\n",
    "            from scipy.stats import gaussian_kde\n",
    "            kde = gaussian_kde(data)\n",
    "            x_range = np.linspace(data.min(), data.max(), 100)\n",
    "            ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ax.set_xlabel(col, fontsize=10)\n",
    "        ax.set_ylabel('Densidade', fontsize=10)\n",
    "        ax.set_title(f'Distribuição: {col}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Adicionar estatísticas no gráfico\n",
    "        stats_text = f\"μ={data.mean():.2f}\\nσ={data.std():.2f}\\nSkew={stats.skew(data):.2f}\"\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', fontsize=9, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'estatisticas', '02_distribuicoes.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Gráfico de distribuições salvo em: estatisticas/02_distribuicoes.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TENDÊNCIAS E TAXAS DE CRESCIMENTO\n",
    "# ==============================================================================\n",
    "print(\"\\n5. TENDÊNCIAS E TAXAS DE CRESCIMENTO\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "def calcular_tendencia(serie, datas):\n",
    "    \"\"\"Calcula tendência linear e taxa de crescimento\"\"\"\n",
    "    serie_clean = serie.dropna()\n",
    "    x = np.arange(len(serie_clean))\n",
    "\n",
    "    # Regressão linear\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, serie_clean)\n",
    "\n",
    "    # CAGR (Compound Annual Growth Rate)\n",
    "    if serie_clean.iloc[0] > 0 and serie_clean.iloc[-1] > 0:\n",
    "        n_years = len(serie_clean) / 12\n",
    "        cagr = (pow(serie_clean.iloc[-1] / serie_clean.iloc[0], 1/n_years) - 1) * 100\n",
    "    else:\n",
    "        cagr = np.nan\n",
    "\n",
    "    return {\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'r_squared': r_value**2,\n",
    "        'p_value': p_value,\n",
    "        'cagr_%': cagr,\n",
    "        'crescimento_total_%': ((serie_clean.iloc[-1] - serie_clean.iloc[0]) / serie_clean.iloc[0] * 100) if serie_clean.iloc[0] > 0 else np.nan\n",
    "    }\n",
    "\n",
    "# Calcular tendências\n",
    "tendencias = []\n",
    "for col in ['capacidade_eolica_ne_mw', 'geracao_eolica_ne_mwmed', 'carga_ne_mwmed',\n",
    "            'demanda_sin_mwmed', 'vel_vento_100m_ne_media', 'capacity_factor_ne']:\n",
    "    if col in df.columns:\n",
    "        tend = calcular_tendencia(df[col], df['data'])\n",
    "        tend['variavel'] = col\n",
    "        tendencias.append(tend)\n",
    "\n",
    "df_tendencias = pd.DataFrame(tendencias)\n",
    "df_tendencias = df_tendencias[['variavel', 'slope', 'intercept', 'r_squared', 'p_value', 'cagr_%', 'crescimento_total_%']]\n",
    "df_tendencias.to_csv(os.path.join(EDA_DIR, 'estatisticas', '03_tendencias.csv'), index=False)\n",
    "\n",
    "print(f\"✓ Análise de tendências salva em: estatisticas/03_tendencias.csv\")\n",
    "\n",
    "# Taxas de crescimento YoY (Year-over-Year)\n",
    "df['ano_mes'] = df['data'].dt.to_period('M')\n",
    "df['capacidade_yoy'] = df['capacidade_eolica_ne_mw'].pct_change(12) * 100\n",
    "df['geracao_yoy'] = df['geracao_eolica_ne_mwmed'].pct_change(12) * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Capacidade YoY\n",
    "axes[0].plot(df['data'], df['capacidade_yoy'], marker='o', linewidth=2, color='navy')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0].set_ylabel('Crescimento YoY (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Taxa de Crescimento Year-over-Year - Capacidade Eólica NE', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Geração YoY\n",
    "axes[1].plot(df['data'], df['geracao_yoy'], marker='o', linewidth=2, color='darkgreen')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Data', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Crescimento YoY (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Taxa de Crescimento Year-over-Year - Geração Eólica NE', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'estatisticas', '04_crescimento_yoy.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Gráfico de crescimento YoY salvo em: estatisticas/04_crescimento_yoy.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. SAZONALIDADE POR MÊS DO ANO\n",
    "# ==============================================================================\n",
    "print(\"\\n6. SAZONALIDADE POR MÊS DO ANO\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Criar boxplots por mês\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "variaveis_sazonalidade = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'capacity_factor_ne',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'carga_ne_mwmed',\n",
    "    'penetracao_eolica_ne',\n",
    "    'temp_celsius_ne'\n",
    "]\n",
    "\n",
    "for i, col in enumerate(variaveis_sazonalidade):\n",
    "    if col in df.columns and i < len(axes):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Boxplot por mês\n",
    "        df.boxplot(column=col, by='mes', ax=ax, grid=True)\n",
    "        ax.set_xlabel('Mês', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel(col, fontsize=11)\n",
    "        ax.set_title(f'Sazonalidade Mensal: {col}', fontsize=12, fontweight='bold')\n",
    "        ax.get_figure().suptitle('')  # Remove título automático do pandas\n",
    "\n",
    "        # Adicionar linha de média mensal\n",
    "        medias_mensais = df.groupby('mes')[col].mean()\n",
    "        ax.plot(range(1, 13), medias_mensais, 'r-o', linewidth=2, markersize=8, label='Média')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'sazonalidade', '01_boxplots_mensais.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Boxplots mensais salvos em: sazonalidade/01_boxplots_mensais.png\")\n",
    "\n",
    "# Perfis sazonais (médias mensais)\n",
    "perfis_sazonais = df.groupby('mes')[variaveis_sazonalidade].mean()\n",
    "perfis_sazonais.to_csv(os.path.join(EDA_DIR, 'sazonalidade', '02_perfis_sazonais.csv'))\n",
    "\n",
    "# Gráfico de perfis sazonais\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "meses_nomes = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
    "\n",
    "for i, col in enumerate(variaveis_sazonalidade):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        perfil = perfis_sazonais[col]\n",
    "\n",
    "        ax.plot(range(1, 13), perfil, marker='o', linewidth=3, markersize=10, color='darkblue')\n",
    "        ax.fill_between(range(1, 13), perfil, alpha=0.3)\n",
    "        ax.set_xlabel('Mês', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Média', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(range(1, 13))\n",
    "        ax.set_xticklabels(meses_nomes, rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Destacar máximo e mínimo\n",
    "        idx_max = perfil.idxmax()\n",
    "        idx_min = perfil.idxmin()\n",
    "        ax.scatter([idx_max], [perfil.loc[idx_max]], color='red', s=200, zorder=5, label=f'Max: {meses_nomes[idx_max-1]}')\n",
    "        ax.scatter([idx_min], [perfil.loc[idx_min]], color='green', s=200, zorder=5, label=f'Min: {meses_nomes[idx_min-1]}')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'sazonalidade', '03_perfis_sazonais.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Perfis sazonais salvos em: sazonalidade/\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. HEATMAPS ANO × MÊS\n",
    "# ==============================================================================\n",
    "print(\"\\n7. HEATMAPS ANO × MÊS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "variaveis_heatmap = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'carga_ne_mwmed',\n",
    "    'capacity_factor_ne',\n",
    "    'penetracao_eolica_ne',\n",
    "    'corte_eolica_ne_mwmed'\n",
    "]\n",
    "\n",
    "for col in variaveis_heatmap:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # Criar pivot table\n",
    "    pivot = df.pivot_table(values=col, index='ano', columns='mes', aggfunc='mean')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd', linewidths=0.5,\n",
    "                cbar_kws={'label': col}, ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Mês', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Ano', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Heatmap Ano × Mês: {col}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticklabels(meses_nomes, rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EDA_DIR, 'heatmaps', f'{col}_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✓ {len(variaveis_heatmap)} heatmaps salvos em: heatmaps/\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. DECOMPOSIÇÃO DE SÉRIES TEMPORAIS\n",
    "# ==============================================================================\n",
    "print(\"\\n8. DECOMPOSIÇÃO DE SÉRIES TEMPORAIS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "variaveis_decomposicao = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'capacity_factor_ne',\n",
    "    'carga_ne_mwmed'\n",
    "]\n",
    "\n",
    "for col in variaveis_decomposicao:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Preparar série\n",
    "        serie = df.set_index('data')[col].dropna()\n",
    "\n",
    "        if len(serie) < 24:  # Precisa de pelo menos 2 anos\n",
    "            continue\n",
    "\n",
    "        # Decomposição STL (Seasonal-Trend decomposition using Loess)\n",
    "        from statsmodels.tsa.seasonal import STL\n",
    "        stl = STL(serie, seasonal=13)  # seasonal = 13 para capturar ciclo anual\n",
    "        result = stl.fit()\n",
    "\n",
    "        # Plotar\n",
    "        fig = result.plot()\n",
    "        fig.set_size_inches(16, 10)\n",
    "        fig.suptitle(f'Decomposição STL: {col}', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(EDA_DIR, 'decomposicao', f'{col}_stl.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Salvar componentes\n",
    "        componentes = pd.DataFrame({\n",
    "            'data': serie.index,\n",
    "            'observado': serie.values,\n",
    "            'tendencia': result.trend,\n",
    "            'sazonalidade': result.seasonal,\n",
    "            'residuo': result.resid\n",
    "        })\n",
    "        componentes.to_csv(os.path.join(EDA_DIR, 'decomposicao', f'{col}_componentes.csv'), index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Aviso: Não foi possível decompor {col}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"✓ Decomposições STL salvas em: decomposicao/\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. ACF/PACF E CORRELAÇÕES CRUZADAS\n",
    "# ==============================================================================\n",
    "print(\"\\n9. ACF/PACF E CORRELAÇÕES CRUZADAS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "variaveis_acf = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'capacity_factor_ne'\n",
    "]\n",
    "\n",
    "for col in variaveis_acf:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    serie = df[col].dropna()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "    # ACF\n",
    "    plot_acf(serie, lags=40, ax=axes[0])\n",
    "    axes[0].set_title(f'Autocorrelação (ACF): {col}', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # PACF\n",
    "    plot_pacf(serie, lags=40, ax=axes[1])\n",
    "    axes[1].set_title(f'Autocorrelação Parcial (PACF): {col}', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EDA_DIR, 'correlacoes', f'{col}_acf_pacf.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"✓ Gráficos ACF/PACF salvos em: correlacoes/\")\n",
    "\n",
    "# Correlação cruzada entre geração e vento com lags\n",
    "if 'geracao_eolica_ne_mwmed' in df.columns and 'vel_vento_100m_ne_media' in df.columns:\n",
    "    serie1 = df['geracao_eolica_ne_mwmed'].dropna()\n",
    "    serie2 = df['vel_vento_100m_ne_media'].dropna()\n",
    "\n",
    "    # Alinhar séries\n",
    "    min_len = min(len(serie1), len(serie2))\n",
    "    serie1 = serie1[:min_len]\n",
    "    serie2 = serie2[:min_len]\n",
    "\n",
    "    # Calcular cross-correlation\n",
    "    cross_corr = [np.corrcoef(serie1[lag:], serie2[:len(serie2)-lag])[0, 1]\n",
    "                  for lag in range(0, 13)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(13), cross_corr, color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Lag (meses)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Correlação', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Correlação Cruzada: Geração Eólica × Velocidade do Vento', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Adicionar valores\n",
    "    for i, v in enumerate(cross_corr):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EDA_DIR, 'correlacoes', 'cross_correlation_geracao_vento.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✓ Correlação cruzada salva em: correlacoes/cross_correlation_geracao_vento.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. ANÁLISE DE EXTREMOS\n",
    "# ==============================================================================\n",
    "print(\"\\n10. ANÁLISE DE EXTREMOS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "extremos_report = []\n",
    "\n",
    "for col in ['geracao_eolica_ne_mwmed', 'vel_vento_100m_ne_media',\n",
    "            'capacity_factor_ne', 'carga_ne_mwmed']:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # Encontrar máximos e mínimos\n",
    "    idx_max = df[col].idxmax()\n",
    "    idx_min = df[col].idxmin()\n",
    "\n",
    "    extremos_report.append({\n",
    "        'variavel': col,\n",
    "        'valor_max': df.loc[idx_max, col],\n",
    "        'data_max': df.loc[idx_max, 'data'].strftime('%Y-%m'),\n",
    "        'valor_min': df.loc[idx_min, col],\n",
    "        'data_min': df.loc[idx_min, 'data'].strftime('%Y-%m'),\n",
    "    })\n",
    "\n",
    "df_extremos = pd.DataFrame(extremos_report)\n",
    "df_extremos.to_csv(os.path.join(EDA_DIR, 'extremos', '01_recordes_mensais.csv'), index=False)\n",
    "\n",
    "print(f\"✓ Análise de extremos salva em: extremos/01_recordes_mensais.csv\")\n",
    "\n",
    "# Distribuição de extremos ao longo do tempo\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "variaveis_extremos = ['geracao_eolica_ne_mwmed', 'vel_vento_100m_ne_media',\n",
    "                      'capacity_factor_ne', 'carga_ne_mwmed']\n",
    "\n",
    "for i, col in enumerate(variaveis_extremos):\n",
    "    if col in df.columns and i < len(axes):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Plotar série\n",
    "        ax.plot(df['data'], df[col], linewidth=1.5, color='gray', alpha=0.7)\n",
    "\n",
    "        # Destacar top 5% e bottom 5%\n",
    "        threshold_high = df[col].quantile(0.95)\n",
    "        threshold_low = df[col].quantile(0.05)\n",
    "\n",
    "        extremos_high = df[df[col] >= threshold_high]\n",
    "        extremos_low = df[df[col] <= threshold_low]\n",
    "\n",
    "        ax.scatter(extremos_high['data'], extremos_high[col],\n",
    "                  color='red', s=80, zorder=5, label='Top 5%', alpha=0.8)\n",
    "        ax.scatter(extremos_low['data'], extremos_low[col],\n",
    "                  color='blue', s=80, zorder=5, label='Bottom 5%', alpha=0.8)\n",
    "\n",
    "        ax.axhline(y=threshold_high, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax.axhline(y=threshold_low, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "        ax.set_xlabel('Data', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel(col, fontsize=11)\n",
    "        ax.set_title(f'Eventos Extremos: {col}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'extremos', '02_eventos_extremos.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Gráfico de eventos extremos salvo em: extremos/02_eventos_extremos.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 11. MATRIZ DE CORRELAÇÃO E SCATTER PLOTS\n",
    "# ==============================================================================\n",
    "print(\"\\n11. MATRIZ DE CORRELAÇÃO E SCATTER PLOTS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Selecionar variáveis principais para correlação\n",
    "variaveis_correlacao = [\n",
    "    'geracao_eolica_ne_mwmed',\n",
    "    'capacidade_eolica_ne_mw',\n",
    "    'vel_vento_100m_ne_media',\n",
    "    'capacity_factor_ne',\n",
    "    'carga_ne_mwmed',\n",
    "    'demanda_sin_mwmed',\n",
    "    'penetracao_eolica_ne',\n",
    "    'corte_eolica_ne_mwmed',\n",
    "    'temp_celsius_ne',\n",
    "    'densidade_ar_ne_media',\n",
    "    'potencia_vento_100m_ne',\n",
    "    'variabilidade_vento_100m'\n",
    "]\n",
    "\n",
    "# Filtrar variáveis existentes\n",
    "variaveis_correlacao = [v for v in variaveis_correlacao if v in df.columns]\n",
    "\n",
    "# Matriz de correlação\n",
    "df_corr = df[variaveis_correlacao].corr()\n",
    "\n",
    "# Plotar matriz de correlação\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
    "sns.heatmap(df_corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, linewidths=1, cbar_kws={'label': 'Correlação'}, ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "ax.set_title('Matriz de Correlação - Variáveis Principais', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'correlacoes', '01_matriz_correlacao.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Salvar matriz em CSV\n",
    "df_corr.to_csv(os.path.join(EDA_DIR, 'correlacoes', '02_matriz_correlacao.csv'))\n",
    "\n",
    "print(f\"✓ Matriz de correlação salva em: correlacoes/\")\n",
    "\n",
    "# Scatter plots das correlações mais fortes\n",
    "top_correlacoes = []\n",
    "for i in range(len(df_corr.columns)):\n",
    "    for j in range(i+1, len(df_corr.columns)):\n",
    "        top_correlacoes.append({\n",
    "            'var1': df_corr.columns[i],\n",
    "            'var2': df_corr.columns[j],\n",
    "            'correlacao': df_corr.iloc[i, j]\n",
    "        })\n",
    "\n",
    "df_top_corr = pd.DataFrame(top_correlacoes)\n",
    "df_top_corr = df_top_corr.sort_values('correlacao', key=abs, ascending=False).head(6)\n",
    "\n",
    "# Plotar scatter plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, row in enumerate(df_top_corr.itertuples()):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "\n",
    "        var1 = row.var1\n",
    "        var2 = row.var2\n",
    "        corr = row.correlacao\n",
    "\n",
    "        ax.scatter(df[var1], df[var2], alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "        # Linha de tendência\n",
    "        z = np.polyfit(df[var1].dropna(), df[var2].dropna(), 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(df[var1].min(), df[var1].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Corr={corr:.3f}')\n",
    "\n",
    "        ax.set_xlabel(var1, fontsize=10)\n",
    "        ax.set_ylabel(var2, fontsize=10)\n",
    "        ax.set_title(f'{var1} × {var2}', fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'correlacoes', '03_scatter_top_correlacoes.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Scatter plots salvos em: correlacoes/03_scatter_top_correlacoes.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 12. INDICADORES DERIVADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n12. INDICADORES DERIVADOS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Fator de utilização já existe (capacity_factor_ne)\n",
    "# Vamos criar outros indicadores\n",
    "\n",
    "# Excedente/Pressão do Sistema\n",
    "if 'geracao_eolica_ne_mwmed' in df.columns and 'carga_ne_mwmed' in df.columns:\n",
    "    df['excedente_sistema'] = df['geracao_eolica_ne_mwmed'] - df['carga_ne_mwmed']\n",
    "    df['ratio_geracao_carga'] = df['geracao_eolica_ne_mwmed'] / df['carga_ne_mwmed']\n",
    "\n",
    "# Intensidade de vento (desvio da média)\n",
    "if 'vel_vento_100m_ne_media' in df.columns:\n",
    "    media_vento_historica = df['vel_vento_100m_ne_media'].mean()\n",
    "    df['desvio_vento_percentual'] = ((df['vel_vento_100m_ne_media'] - media_vento_historica) / media_vento_historica) * 100\n",
    "\n",
    "# Eficiência de conversão (proxy)\n",
    "if 'geracao_eolica_ne_mwmed' in df.columns and 'potencia_vento_100m_ne' in df.columns:\n",
    "    df['eficiencia_conversao_proxy'] = df['geracao_eolica_ne_mwmed'] / (df['potencia_vento_100m_ne'] * df['capacidade_eolica_ne_mw'])\n",
    "\n",
    "# Plotar indicadores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Excedente do sistema\n",
    "if 'excedente_sistema' in df.columns:\n",
    "    axes[0, 0].plot(df['data'], df['excedente_sistema'], linewidth=2)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "    axes[0, 0].fill_between(df['data'], df['excedente_sistema'], 0,\n",
    "                             where=df['excedente_sistema']>=0, alpha=0.3, color='green', label='Excedente')\n",
    "    axes[0, 0].fill_between(df['data'], df['excedente_sistema'], 0,\n",
    "                             where=df['excedente_sistema']<0, alpha=0.3, color='red', label='Déficit')\n",
    "    axes[0, 0].set_ylabel('MWmed', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title('Excedente/Déficit: Geração Eólica - Carga NE', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ratio Geração/Carga\n",
    "if 'ratio_geracao_carga' in df.columns:\n",
    "    axes[0, 1].plot(df['data'], df['ratio_geracao_carga'], linewidth=2, color='purple')\n",
    "    axes[0, 1].axhline(y=1, color='red', linestyle='--', linewidth=1, label='Equilíbrio')\n",
    "    axes[0, 1].set_ylabel('Ratio', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title('Ratio: Geração Eólica / Carga NE', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Desvio do vento\n",
    "if 'desvio_vento_percentual' in df.columns:\n",
    "    axes[1, 0].plot(df['data'], df['desvio_vento_percentual'], linewidth=2, color='darkblue')\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "    axes[1, 0].fill_between(df['data'], df['desvio_vento_percentual'], 0, alpha=0.3)\n",
    "    axes[1, 0].set_ylabel('Desvio (%)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title('Desvio da Média Histórica: Velocidade do Vento', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Capacity Factor ao longo do tempo\n",
    "if 'capacity_factor_ne' in df.columns:\n",
    "    axes[1, 1].plot(df['data'], df['capacity_factor_ne'], linewidth=2, color='darkgreen')\n",
    "    axes[1, 1].axhline(y=df['capacity_factor_ne'].mean(), color='red', linestyle='--',\n",
    "                       linewidth=1, label=f'Média={df[\"capacity_factor_ne\"].mean():.1f}%')\n",
    "    axes[1, 1].set_ylabel('Capacity Factor (%)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title('Fator de Capacidade Mensal NE', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'indicadores', '01_indicadores_derivados.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Indicadores derivados salvos em: indicadores/\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 13. HIPÓTESES E DRIVERS DE VARIABILIDADE\n",
    "# ==============================================================================\n",
    "print(\"\\n13. HIPÓTESES E DRIVERS DE VARIABILIDADE\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "with open(os.path.join(EDA_DIR, 'hipoteses', '01_analise_drivers.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"ANÁLISE DE DRIVERS DE VARIABILIDADE EÓLICA - NE DO BRASIL\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "\n",
    "    # Hipótese 1: Sazonalidade\n",
    "    f.write(\"HIPÓTESE 1: SAZONALIDADE DO VENTO\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    perfil_vento = df.groupby('mes')['vel_vento_100m_ne_media'].mean()\n",
    "    mes_max_vento = perfil_vento.idxmax()\n",
    "    mes_min_vento = perfil_vento.idxmin()\n",
    "    f.write(f\"• Mês com maior vento: {meses_nomes[mes_max_vento-1]} ({perfil_vento.max():.2f} m/s)\\n\")\n",
    "    f.write(f\"• Mês com menor vento: {meses_nomes[mes_min_vento-1]} ({perfil_vento.min():.2f} m/s)\\n\")\n",
    "    f.write(f\"• Amplitude sazonal: {perfil_vento.max() - perfil_vento.min():.2f} m/s\\n\")\n",
    "    f.write(f\"• Coeficiente de variação sazonal: {(perfil_vento.std() / perfil_vento.mean() * 100):.2f}%\\n\\n\")\n",
    "\n",
    "    perfil_cf = df.groupby('mes')['capacity_factor_ne'].mean()\n",
    "    mes_max_cf = perfil_cf.idxmax()\n",
    "    mes_min_cf = perfil_cf.idxmin()\n",
    "    f.write(f\"• Mês com maior CF: {meses_nomes[mes_max_cf-1]} ({perfil_cf.max():.2f}%)\\n\")\n",
    "    f.write(f\"• Mês com menor CF: {meses_nomes[mes_min_cf-1]} ({perfil_cf.min():.2f}%)\\n\")\n",
    "    f.write(f\"• Amplitude sazonal CF: {perfil_cf.max() - perfil_cf.min():.2f}%\\n\\n\")\n",
    "\n",
    "    f.write(\"CONCLUSÃO: Forte sazonalidade observada, com ventos mais intensos no segundo semestre.\\n\")\n",
    "    f.write(\"Isso está alinhado com os padrões climáticos do NE (ventos alísios).\\n\\n\\n\")\n",
    "\n",
    "    # Hipótese 2: Anos anômalos\n",
    "    f.write(\"HIPÓTESE 2: ANOS ANÔMALOS\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    cf_por_ano = df.groupby('ano')['capacity_factor_ne'].mean().sort_values(ascending=False)\n",
    "    f.write(\"Capacity Factor médio por ano (ranking):\\n\")\n",
    "    for i, (ano, cf) in enumerate(cf_por_ano.items(), 1):\n",
    "        f.write(f\"  {i}. {ano}: {cf:.2f}%\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Identificar anos com CF > +1 std ou < -1 std\n",
    "    cf_mean = cf_por_ano.mean()\n",
    "    cf_std = cf_por_ano.std()\n",
    "    anos_alto = cf_por_ano[cf_por_ano > cf_mean + cf_std]\n",
    "    anos_baixo = cf_por_ano[cf_por_ano < cf_mean - cf_std]\n",
    "\n",
    "    if len(anos_alto) > 0:\n",
    "        f.write(f\"Anos com CF ACIMA DA MÉDIA (+1σ):\\n\")\n",
    "        for ano, cf in anos_alto.items():\n",
    "            f.write(f\"  • {ano}: {cf:.2f}% (desvio: +{cf-cf_mean:.2f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    if len(anos_baixo) > 0:\n",
    "        f.write(f\"Anos com CF ABAIXO DA MÉDIA (-1σ):\\n\")\n",
    "        for ano, cf in anos_baixo.items():\n",
    "            f.write(f\"  • {ano}: {cf:.2f}% (desvio: {cf-cf_mean:.2f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"CONCLUSÃO: Variabilidade interanual significativa, possivelmente associada a\\n\")\n",
    "    f.write(\"fenômenos climáticos de larga escala (El Niño, La Niña, etc.).\\n\\n\\n\")\n",
    "\n",
    "    # Hipótese 3: Curtailment\n",
    "    f.write(\"HIPÓTESE 3: CURTAILMENT E RESTRIÇÕES DE ESCOAMENTO\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    curtailment_medio = df['corte_eolica_ne_mwmed'].mean()\n",
    "    curtailment_max = df['corte_eolica_ne_mwmed'].max()\n",
    "    idx_curt_max = df['corte_eolica_ne_mwmed'].idxmax()\n",
    "\n",
    "    f.write(f\"• Curtailment médio: {curtailment_medio:.2f} MWmed\\n\")\n",
    "    f.write(f\"• Curtailment máximo: {curtailment_max:.2f} MWmed (em {df.loc[idx_curt_max, 'data'].strftime('%Y-%m')})\\n\")\n",
    "    f.write(f\"• Meses com curtailment > 100 MWmed: {(df['corte_eolica_ne_mwmed'] > 100).sum()} meses\\n\\n\")\n",
    "\n",
    "    # Correlação entre penetração e curtailment\n",
    "    if 'penetracao_eolica_ne' in df.columns:\n",
    "        corr_pen_curt = df[['penetracao_eolica_ne', 'corte_eolica_ne_mwmed']].corr().iloc[0, 1]\n",
    "        f.write(f\"• Correlação penetração × curtailment: {corr_pen_curt:.3f}\\n\")\n",
    "        f.write(\"CONCLUSÃO: Curtailment aumenta com penetração eólica, sugerindo restrições\\n\")\n",
    "        f.write(\"de escoamento e necessidade de expansão da transmissão.\\n\\n\\n\")\n",
    "\n",
    "    # Hipótese 4: Crescimento da capacidade\n",
    "    f.write(\"HIPÓTESE 4: IMPACTO DO CRESCIMENTO DA CAPACIDADE\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    cap_inicial = df['capacidade_eolica_ne_mw'].iloc[0]\n",
    "    cap_final = df['capacidade_eolica_ne_mw'].iloc[-1]\n",
    "    crescimento_total = ((cap_final - cap_inicial) / cap_inicial) * 100\n",
    "\n",
    "    f.write(f\"• Capacidade inicial ({df['data'].iloc[0].strftime('%Y-%m')}): {cap_inicial:.0f} MW\\n\")\n",
    "    f.write(f\"• Capacidade final ({df['data'].iloc[-1].strftime('%Y-%m')}): {cap_final:.0f} MW\\n\")\n",
    "    f.write(f\"• Crescimento total: {crescimento_total:.1f}%\\n\")\n",
    "    f.write(f\"• CAGR: {df_tendencias[df_tendencias['variavel']=='capacidade_eolica_ne_mw']['cagr_%'].values[0]:.2f}%/ano\\n\\n\")\n",
    "\n",
    "    f.write(\"CONCLUSÃO: Crescimento acelerado da capacidade, com potencial impacto na\\n\")\n",
    "    f.write(\"confiabilidade do sistema e necessidade de flexibilidade operacional.\\n\\n\")\n",
    "\n",
    "print(f\"✓ Análise de drivers salva em: hipoteses/01_analise_drivers.txt\")\n",
    "\n",
    "# Gráfico final: Evolução conjunta dos principais drivers\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14), sharex=True)\n",
    "\n",
    "# Capacidade e Geração\n",
    "ax1 = axes[0]\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1.plot(df['data'], df['capacidade_eolica_ne_mw'], 'b-', linewidth=2, label='Capacidade (MW)')\n",
    "ax1_twin.plot(df['data'], df['geracao_eolica_ne_mwmed'], 'g-', linewidth=2, label='Geração (MWmed)')\n",
    "ax1.set_ylabel('Capacidade (MW)', fontsize=11, fontweight='bold', color='b')\n",
    "ax1_twin.set_ylabel('Geração (MWmed)', fontsize=11, fontweight='bold', color='g')\n",
    "ax1.set_title('Evolução Conjunta: Capacidade vs Geração', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1_twin.legend(loc='upper right')\n",
    "\n",
    "# Vento e CF\n",
    "ax2 = axes[1]\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2.plot(df['data'], df['vel_vento_100m_ne_media'], 'darkblue', linewidth=2, label='Vento 100m (m/s)')\n",
    "ax2_twin.plot(df['data'], df['capacity_factor_ne'], 'orange', linewidth=2, label='Capacity Factor (%)')\n",
    "ax2.set_ylabel('Vento (m/s)', fontsize=11, fontweight='bold', color='darkblue')\n",
    "ax2_twin.set_ylabel('CF (%)', fontsize=11, fontweight='bold', color='orange')\n",
    "ax2.set_title('Evolução Conjunta: Vento vs Capacity Factor', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2_twin.legend(loc='upper right')\n",
    "\n",
    "# Penetração e Curtailment\n",
    "ax3 = axes[2]\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3.plot(df['data'], df['penetracao_eolica_ne'], 'purple', linewidth=2, label='Penetração NE (%)')\n",
    "ax3_twin.plot(df['data'], df['corte_eolica_ne_mwmed'], 'red', linewidth=2, label='Curtailment (MWmed)')\n",
    "ax3.set_ylabel('Penetração (%)', fontsize=11, fontweight='bold', color='purple')\n",
    "ax3_twin.set_ylabel('Curtailment (MWmed)', fontsize=11, fontweight='bold', color='red')\n",
    "ax3.set_title('Evolução Conjunta: Penetração vs Curtailment', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3_twin.legend(loc='upper right')\n",
    "\n",
    "# Demanda NE vs SIN\n",
    "ax4 = axes[3]\n",
    "ax4_twin = ax4.twinx()\n",
    "ax4.plot(df['data'], df['carga_ne_mwmed'], 'teal', linewidth=2, label='Carga NE (MWmed)')\n",
    "ax4_twin.plot(df['data'], df['demanda_sin_mwmed'], 'brown', linewidth=2, label='Demanda SIN (MWmed)')\n",
    "ax4.set_xlabel('Data', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Carga NE (MWmed)', fontsize=11, fontweight='bold', color='teal')\n",
    "ax4_twin.set_ylabel('Demanda SIN (MWmed)', fontsize=11, fontweight='bold', color='brown')\n",
    "ax4.set_title('Evolução Conjunta: Carga NE vs Demanda SIN', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend(loc='upper left')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EDA_DIR, 'hipoteses', '02_evolucao_conjunta_drivers.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"✓ Gráfico de evolução conjunta salvo em: hipoteses/02_evolucao_conjunta_drivers.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RELATÓRIO FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EDA COMPLETA FINALIZADA COM SUCESSO!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nResumo dos outputs gerados:\")\n",
    "print(f\"  • Diretório principal: {EDA_DIR}\")\n",
    "print(f\"  • Subdiretórios: {len(subdirs)}\")\n",
    "print(f\"  • Gráficos gerados: ~40+\")\n",
    "print(f\"  • Tabelas CSV: ~10+\")\n",
    "print(f\"  • Relatórios TXT: 3\")\n",
    "print(f\"\\nPróximas etapas recomendadas:\")\n",
    "print(f\"  1. Revisar relatórios de validação e integridade\")\n",
    "print(f\"  2. Analisar séries temporais e identificar anomalias\")\n",
    "print(f\"  3. Investigar correlações fortes entre variáveis\")\n",
    "print(f\"  4. Validar hipóteses sobre drivers de variabilidade\")\n",
    "print(f\"  5. Preparar features para modelagem com base nas análises\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e4aae",
   "metadata": {},
   "source": [
    "## Pipeline dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50173f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline do Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline do XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38cbfa",
   "metadata": {},
   "source": [
    "## Estatisticas Finais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
